{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98e5e870-3d99-41ce-afbc-9707314979e3",
   "metadata": {},
   "source": [
    "# STEREOSET\n",
    "**StereoSet** è un dataset progettato per misurare e rilevare i **bias nei modelli di linguaggio** rispetto a diverse categorie, come **genere, razza, religione, e orientamento sessuale**. Il dataset è suddiviso in **tre diverse sezioni** — **Intrasentence**, **Intersentence**, e **Commonsense** — che servono a valutare i bias in contesti differenti e con diversi tipi di frasi.\n",
    "\n",
    "### Le sezioni di StereoSet:\n",
    "\n",
    "#### 1. **Intrasentence**:\n",
    "- **Descrizione**: Questa sezione contiene frasi singole in cui il bias si trova **all'interno della frase stessa**. L'obiettivo è valutare se i modelli di linguaggio **preferiscono stereotipi** o frasi che trasmettono bias all'interno di una singola frase, rispetto a opzioni neutre o contrarie.\n",
    "- **Struttura**: Ogni esempio contiene una frase \"context\" seguita da tre possibili completamenti:\n",
    "  1. **Stereotypical (stereotipico)**: Una continuazione che riflette un bias o stereotipo.\n",
    "  2. **Anti-stereotypical (anti-stereotipico)**: Una continuazione che contrasta lo stereotipo.\n",
    "  3. **Unrelated (non correlato)**: Una continuazione che non è rilevante o non si collega direttamente alla frase.\n",
    "\n",
    "**Esempio**:\n",
    "- **Contesto**: \"John loves fixing cars. He must be a...\"\n",
    "  - **Stereotypical completamento**: \"mechanic.\"\n",
    "  - **Anti-stereotypical completamento**: \"teacher.\"\n",
    "  - **Unrelated completamento**: \"banana.\"\n",
    "\n",
    "In questo esempio, il modello viene testato per vedere se predilige un completamento stereotipico, come \"mechanic\" (stereotipo di genere che associa gli uomini ai meccanici), rispetto a \"teacher\" o \"banana\".\n",
    "\n",
    "#### 2. **Intersentence**:\n",
    "- **Descrizione**: Questa sezione contiene **due frasi consecutive**, in cui il bias è presente **nella relazione tra le due frasi**. Qui, il bias può essere testato rispetto al modo in cui il modello collega il contesto di una frase alla successiva.\n",
    "- **Struttura**: Come in Intrasentence, anche qui ci sono tre opzioni per la seconda frase:\n",
    "  1. **Stereotypical**: Una seconda frase che segue uno stereotipo.\n",
    "  2. **Anti-stereotypical**: Una seconda frase che contrasta lo stereotipo.\n",
    "  3. **Unrelated**: Una seconda frase non collegata.\n",
    "\n",
    "**Esempio**:\n",
    "- **Frase 1**: \"Jane works as a software engineer.\"\n",
    "  - **Stereotypical seconda frase**: \"She must be bad at coding.\"\n",
    "  - **Anti-stereotypical seconda frase**: \"She is great at coding.\"\n",
    "  - **Unrelated seconda frase**: \"She loves painting on weekends.\"\n",
    "\n",
    "In questo caso, il modello è testato per verificare se associa in modo stereotipato una donna ingegnere a una valutazione negativa delle sue capacità tecniche.\n",
    "\n",
    "#### 3. **Commonsense**:\n",
    "- **Descrizione**: Questa sezione valuta le conoscenze generali e il ragionamento **basato sul buon senso**, senza necessariamente includere stereotipi. Questo serve a garantire che il modello non solo eviti i bias, ma che abbia anche una buona comprensione del **buon senso**.\n",
    "- **Struttura**: Ogni esempio commonsense presenta una frase e tre possibili completamenti. Qui, l'obiettivo è vedere se il modello può scegliere il completamento che ha più senso a livello logico.\n",
    "  1. **True**: Un completamento che ha senso dal punto di vista del buon senso.\n",
    "  2. **False**: Un completamento che non ha senso.\n",
    "  3. **Unrelated**: Un completamento che non è rilevante per la frase.\n",
    "\n",
    "**Esempio**:\n",
    "- **Frase**: \"The sun rises in the...\"\n",
    "  - **True completamento**: \"east.\"\n",
    "  - **False completamento**: \"west.\"\n",
    "  - **Unrelated completamento**: \"ocean.\"\n",
    "\n",
    "In questo caso, il modello deve dimostrare di avere un buon livello di conoscenza del mondo reale, scegliendo la risposta corretta (\"east\") rispetto a risposte errate o non correlate.\n",
    "\n",
    "### Obiettivo delle tre sezioni\n",
    "\n",
    "- **Intrasentence** e **Intersentence** sono progettate per rilevare **bias e stereotipi** nelle preferenze del modello, testando se è incline a scegliere frasi stereotipate rispetto a frasi anti-stereotipate o neutrali.\n",
    "- **Commonsense** serve come controllo per garantire che un modello non stia solo evitando bias, ma che possieda anche una buona comprensione del **ragionamento basato sul buon senso**. Questo è importante perché un modello che evita i bias, ma non ha un buon ragionamento, non sarà utile in molte applicazioni pratiche.\n",
    "\n",
    "### Dettagli sui tipi di bias rilevati\n",
    "\n",
    "StereoSet classifica i bias in **quattro categorie principali**:\n",
    "1. **Genere**: Bias legati a ruoli o caratteristiche di genere (es. uomini associati a lavori tecnici, donne a ruoli di cura).\n",
    "2. **Razza**: Bias basati su stereotipi razziali (es. associando determinate capacità o comportamenti a un gruppo etnico specifico).\n",
    "3. **Religione**: Bias riguardanti stereotipi religiosi (es. associando determinate credenze o comportamenti a una religione).\n",
    "4. **Orientamento sessuale**: Bias verso persone di orientamenti sessuali diversi (es. stereotipi sull'omosessualità o l'eterosessualità).\n",
    "\n",
    "### Utilizzo del dataset\n",
    "\n",
    "Puoi usare il dataset per **valutare i modelli di linguaggio** e verificare se tendono a favorire risposte stereotipate o se mostrano segni di bias nei loro completamenti di frasi. La struttura del dataset ti permette di confrontare le risposte stereotipate con quelle anti-stereotipate e di misurare il livello di bias presente nel modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423d50e8-c05f-4959-9a7e-5416f29bae9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from neo4j import GraphDatabase\n",
    "from nltk.corpus import wordnet as wn\n",
    "import nltk\n",
    "\n",
    "# Scarica il corpus WordNet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "# Configura la connessione a Neo4j\n",
    "class Neo4jHandler:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def create_node_if_not_exists(self, term, roles, wikidata_description=None, wordnet_description=None):\n",
    "        \"\"\"\n",
    "        Crea un nodo se non esiste già. Usa il termine come ID del nodo.\n",
    "        Se il nodo esiste già, aggiorna la sua descrizione se disponibile una nuova da Wikidata o WordNet.\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            # Concateniamo le descrizioni se disponibili senza aggiungere prefissi\n",
    "            descriptions = []\n",
    "            if wikidata_description:\n",
    "                descriptions.append(wikidata_description)  # Descrizione da Wikidata\n",
    "            if wordnet_description:\n",
    "                descriptions.append(wordnet_description)  # Descrizione da WordNet\n",
    "            final_description = \" | \".join(descriptions) if descriptions else \"\"\n",
    "    \n",
    "            # Primo step: creiamo o aggiorniamo il nodo. Se esiste, aggiorniamo la descrizione.\n",
    "            query_merge = \"\"\"\n",
    "            MERGE (n:Entity {id: $term})\n",
    "            ON CREATE SET n.label = $label, n.description = $description\n",
    "            ON MATCH SET n.description = CASE \n",
    "                WHEN $description <> '' THEN $description \n",
    "                ELSE n.description \n",
    "            END\n",
    "            RETURN n\n",
    "            \"\"\"\n",
    "            session.run(query_merge, term=term, label='|'.join(roles), description=final_description)\n",
    "    \n",
    "            # Secondo step: aggiorniamo la label solo se non è già inclusa\n",
    "            for role in roles:\n",
    "                query_update_label = \"\"\"\n",
    "                MATCH (n:Entity {id: $term})\n",
    "                WHERE NOT n.label CONTAINS $role\n",
    "                SET n.label = n.label + '|' + $role\n",
    "                RETURN n\n",
    "                \"\"\"\n",
    "                session.run(query_update_label, term=term, role=role)\n",
    "\n",
    "    def create_relationship_if_not_exists(self, term_id, related_entity_id, relationship_type):\n",
    "        \"\"\"\n",
    "        Crea una relazione tra due nodi solo se non esiste già.\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            query = f\"\"\"\n",
    "            MATCH (a:Entity {{id: $term_id}}), (b:Entity {{id: $related_entity_id}})\n",
    "            MERGE (a)-[r:{relationship_type}]->(b)\n",
    "            RETURN r\n",
    "            \"\"\"\n",
    "            session.run(query, term_id=term_id, related_entity_id=related_entity_id)\n",
    "\n",
    "    def verb_exists_in_neo4j(self, verbo):\n",
    "        \"\"\"\n",
    "        Controlla se un nodo per il verbo (lemma) esiste già nel grafo Neo4j.\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            query = \"\"\"\n",
    "            MATCH (n:Entity {id: $verbo})\n",
    "            RETURN n LIMIT 1\n",
    "            \"\"\"\n",
    "            result = session.run(query, verbo=verbo)\n",
    "            return result.single() is not None\n",
    "\n",
    "WIKIDATA_API_URL = \"https://www.wikidata.org/w/api.php\"\n",
    "\n",
    "def search_wikidata(term, limit=20):\n",
    "    \"\"\"\n",
    "    Cerca entità su Wikidata e ritorna id, label e descrizione.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"action\": \"wbsearchentities\",\n",
    "        \"search\": term,\n",
    "        \"language\": \"en\",\n",
    "        \"format\": \"json\",\n",
    "        \"limit\": limit\n",
    "    }\n",
    "    response = requests.get(WIKIDATA_API_URL, params=params)\n",
    "    results = response.json().get(\"search\", [])\n",
    "    \n",
    "    entities = []\n",
    "    for entity in results:\n",
    "        entities.append({\n",
    "            \"id\": entity.get(\"id\"),\n",
    "            \"label\": entity.get(\"label\"),\n",
    "            \"description\": entity.get(\"description\")\n",
    "        })\n",
    "    \n",
    "    return entities\n",
    "\n",
    "def get_entity_data(entity_id):\n",
    "    \"\"\"\n",
    "    Ottiene le proprietà 'instance of' e 'subclass of' per una determinata entità.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"action\": \"wbgetentities\",\n",
    "        \"ids\": entity_id,\n",
    "        \"props\": \"claims\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    response = requests.get(WIKIDATA_API_URL, params=params)\n",
    "    entity_data = response.json().get(\"entities\", {}).get(entity_id, {})\n",
    "    claims = entity_data.get(\"claims\", {})\n",
    "    \n",
    "    instance_of = claims.get(\"P31\", [])\n",
    "    subclass_of = claims.get(\"P279\", [])\n",
    "    \n",
    "    return instance_of, subclass_of\n",
    "\n",
    "def extract_labels_descriptions(property_claims):\n",
    "    \"\"\"\n",
    "    Estrae etichette e descrizioni per ogni entità associata a 'instance of' o 'subclass of'.\n",
    "    \"\"\"\n",
    "    ids = [claim[\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"] for claim in property_claims if \"mainsnak\" in claim and \"datavalue\" in claim[\"mainsnak\"]]\n",
    "    \n",
    "    if not ids:\n",
    "        return []\n",
    "    \n",
    "    ids_str = \"|\".join(ids)\n",
    "    \n",
    "    params = {\n",
    "        \"action\": \"wbgetentities\",\n",
    "        \"ids\": ids_str,\n",
    "        \"props\": \"labels|descriptions\",\n",
    "        \"languages\": \"en\",  # Otteniamo etichette e descrizioni in inglese\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    \n",
    "    response = requests.get(WIKIDATA_API_URL, params=params)\n",
    "    entities = response.json().get(\"entities\", {})\n",
    "    \n",
    "    results = []\n",
    "    for entity_id, entity_info in entities.items():\n",
    "        # Estrai la descrizione dall'entità, se disponibile\n",
    "        label = entity_info.get(\"labels\", {}).get(\"en\", {}).get(\"value\", \"\")\n",
    "        description = entity_info.get(\"descriptions\", {}).get(\"en\", {}).get(\"value\", \"\")\n",
    "        \n",
    "        # Se non c'è descrizione, imposta un valore predefinito o lascialo vuoto\n",
    "        description = description if description else \"\"\n",
    "        \n",
    "        print(\"--->\",label, description)\n",
    "        \n",
    "        results.append({\n",
    "            \"id\": entity_id,\n",
    "            \"label\": label,\n",
    "            \"description\": description  # Usa la descrizione estratta da Wikidata\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def save_to_neo4j(neo4j_handler, term, instance_of, subclass_of, wordnet_description=None):\n",
    "    \"\"\"\n",
    "    Salva i dati estratti su Neo4j e crea relazioni 'instance of' e 'subclass of'.\n",
    "    Usa descrizioni da Wikidata e WordNet, se disponibili.\n",
    "    \"\"\"\n",
    "    # Ottieni la descrizione da Wikidata se disponibile\n",
    "    wikidata_description = None\n",
    "    if instance_of:\n",
    "        wikidata_description = instance_of[0][\"description\"]  # Usa direttamente la descrizione da Wikidata\n",
    "    elif subclass_of:\n",
    "        wikidata_description = subclass_of[0][\"description\"]  # Usa direttamente la descrizione da Wikidata\n",
    "\n",
    "    # Se esiste una descrizione da Wikidata, usiamola\n",
    "    # Aggiungiamo anche la descrizione da WordNet se disponibile\n",
    "    final_description = \"\"\n",
    "    if wikidata_description and wikidata_description != \"No description available\":\n",
    "        final_description += wikidata_description\n",
    "    if wordnet_description:\n",
    "        if final_description:\n",
    "            final_description += \" | \"  # Concatena le descrizioni se entrambe esistono\n",
    "        final_description += wordnet_description\n",
    "\n",
    "    # Passiamo la descrizione combinata (senza prefissi) al nodo\n",
    "    neo4j_handler.create_node_if_not_exists(term, ['ENTITY'], final_description)\n",
    "    \n",
    "    # Inserisci i nodi e le relazioni 'instance of'\n",
    "    for instance in instance_of:\n",
    "        neo4j_handler.create_node_if_not_exists(instance[\"label\"], ['INSTANCE'], instance[\"description\"])\n",
    "        neo4j_handler.create_relationship_if_not_exists(term, instance[\"label\"], \"INSTANCE_OF\")\n",
    "    \n",
    "    # Inserisci i nodi e le relazioni 'subclass of'\n",
    "    for subclass in subclass_of:\n",
    "        neo4j_handler.create_node_if_not_exists(subclass[\"label\"], ['SUBCLASS'], subclass[\"description\"])\n",
    "        neo4j_handler.create_relationship_if_not_exists(term, subclass[\"label\"], \"SUBCLASS_OF\")\n",
    "\n",
    "def enrich_with_synonyms_and_antonyms(neo4j_handler, term):\n",
    "    \"\"\"\n",
    "    Arricchisce il nodo principale con tutti i sinonimi e contrari trovati tramite WordNet.\n",
    "    I sinonimi e contrari vengono tutti collegati direttamente al nodo principale.\n",
    "    \"\"\"\n",
    "    synsets = wn.synsets(term)\n",
    "\n",
    "    if not synsets:\n",
    "        print(f\"Nessun synset trovato per il termine '{term}'\")\n",
    "        return\n",
    "\n",
    "    processed_synonyms = set()  # Cache per tenere traccia dei sinonimi già elaborati\n",
    "\n",
    "    for synset in synsets:\n",
    "        lemmas = synset.lemmas()\n",
    "        \n",
    "        for lemma in lemmas:\n",
    "            synonym = lemma.name()\n",
    "            \n",
    "            # Assicuriamoci di non creare duplicati\n",
    "            if synonym != term and synonym not in processed_synonyms:\n",
    "                processed_synonyms.add(synonym)\n",
    "                # Ottieni la definizione di WordNet per il sinonimo\n",
    "                description = synset.definition() if synset else f\"Synonym of {term}\"\n",
    "                print(f\"Processo sinonimo: {synonym}\")\n",
    "                # Crea il nodo per il sinonimo SENZA label\n",
    "                neo4j_handler.create_node_if_not_exists(synonym, [], description)\n",
    "                neo4j_handler.create_relationship_if_not_exists(term, synonym, \"SYNONYM_OF\")\n",
    "            \n",
    "            antonyms = lemma.antonyms()\n",
    "            \n",
    "            for antonym in antonyms:\n",
    "                antonym_name = antonym.name()\n",
    "                if antonym_name != term and antonym_name not in processed_synonyms:\n",
    "                    processed_synonyms.add(antonym_name)\n",
    "                    # Ottieni la definizione di WordNet per il contrario\n",
    "                    antonym_synset = wn.synsets(antonym_name)\n",
    "                    description = antonym_synset[0].definition() if antonym_synset else f\"Antonym of {term}\"\n",
    "                    print(f\"Processo antonimo: {antonym_name}\")\n",
    "                    # Crea il nodo per il contrario SENZA label\n",
    "                    neo4j_handler.create_node_if_not_exists(antonym_name, [], description)\n",
    "                    neo4j_handler.create_relationship_if_not_exists(term, antonym_name, \"ANTONYM_OF\")\n",
    "\n",
    "def process_verb_with_wordnet(neo4j_handler, verbo, roles):\n",
    "    \"\"\"\n",
    "    Elabora i verbi (lemmi) arricchendoli con sinonimi e contrari da WordNet.\n",
    "    Gestisce anche le relazioni se il verbo esiste già.\n",
    "    \"\"\"\n",
    "    synsets = wn.synsets(verbo, pos=wn.VERB)\n",
    "    description = synsets[0].definition() if synsets else f\"Verb: {verbo}\"\n",
    "\n",
    "    # Controlliamo se il verbo esiste già\n",
    "    if neo4j_handler.verb_exists_in_neo4j(verbo):\n",
    "        print(f\"Verbo '{verbo}' già presente nel grafo, creando comunque le relazioni.\")\n",
    "        # Anche se il nodo esiste, creiamo le relazioni e aggiorniamo i ruoli\n",
    "        neo4j_handler.create_node_if_not_exists(verbo, roles, description)\n",
    "        enrich_with_synonyms_and_antonyms(neo4j_handler, verbo)\n",
    "        return\n",
    "\n",
    "    # Aggiungi il verbo come nodo al grafo\n",
    "    neo4j_handler.create_node_if_not_exists(verbo, roles, description)\n",
    "    \n",
    "    # Aggiungi sinonimi e contrari per il verbo\n",
    "    enrich_with_synonyms_and_antonyms(neo4j_handler, verbo)\n",
    "\n",
    "\n",
    "def process_entity(term, neo4j_handler, limit=50):\n",
    "    \"\"\"\n",
    "    Processa un'entità, arricchendola con dati da Wikidata, e aggiunge sinonimi e contrari da WordNet.\n",
    "    \"\"\"\n",
    "    # Ottieni i dati da Wikidata\n",
    "    entities = search_wikidata(term, limit=limit)\n",
    "    \n",
    "    if not entities:\n",
    "        print(f\"Nessun risultato trovato per il termine '{term}'\")\n",
    "        return\n",
    "    \n",
    "    instance_of_entities = []\n",
    "    subclass_of_entities = []\n",
    "\n",
    "    for entity_data in entities:\n",
    "        entity_id = entity_data[\"id\"]\n",
    "        instance_of_claims, subclass_of_claims = get_entity_data(entity_id)\n",
    "        instance_of_entities.extend(extract_labels_descriptions(instance_of_claims))\n",
    "        subclass_of_entities.extend(extract_labels_descriptions(subclass_of_claims))\n",
    "    \n",
    "    # Ottieni la definizione di WordNet se esiste\n",
    "    wordnet_synsets = wn.synsets(term)\n",
    "    wordnet_description = wordnet_synsets[0].definition() if wordnet_synsets else None\n",
    "\n",
    "    # Salva i dati su Neo4j con entrambe le descrizioni\n",
    "    save_to_neo4j(neo4j_handler, term, instance_of_entities, subclass_of_entities, wordnet_description)\n",
    "\n",
    "    # Aggiungi sinonimi e contrari da WordNet\n",
    "    enrich_with_synonyms_and_antonyms(neo4j_handler, term)\n",
    "\n",
    "# Funzione per estrarre ruoli sintattici, NER e relazioni SVO\n",
    "def extract_syntactic_roles_and_ner(text):\n",
    "    \"\"\"\n",
    "    Estrae soggetti, predicati, oggetti, aggettivi, avverbi, sostantivi, entità nominate e relazioni SVO da un testo.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    soggetti = []\n",
    "    predicati = []\n",
    "    oggetti = []\n",
    "    aggettivi = []\n",
    "    avverbi = []\n",
    "    sostantivi = []\n",
    "    entita_nominate = []\n",
    "    relazioni_svo = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"nsubj\":\n",
    "            soggetti.append(token.text)\n",
    "        if token.pos_ == \"VERB\" or token.pos_ == \"AUX\":\n",
    "            predicati.append(token.text)\n",
    "        if token.dep_ == \"dobj\":\n",
    "            oggetti.append(token.text)\n",
    "        if token.pos_ == \"ADJ\":\n",
    "            aggettivi.append(token.text)\n",
    "        if token.pos_ == \"ADV\":\n",
    "            avverbi.append(token.text)\n",
    "        if token.pos_ == \"NOUN\":\n",
    "            sostantivi.append(token.text)\n",
    "\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"ROOT\":\n",
    "            soggetto = None\n",
    "            oggetto = None\n",
    "            pred_aggettivo = None\n",
    "\n",
    "            for child in token.children:\n",
    "                if child.dep_ in [\"nsubj\", \"nsubjpass\", \"csubj\"]:\n",
    "                    soggetto = child.text\n",
    "\n",
    "            for child in token.children:\n",
    "                if child.dep_ in [\"dobj\", \"pobj\", \"iobj\"]:\n",
    "                    oggetto = child.text\n",
    "                if child.dep_ in [\"acomp\", \"attr\", \"oprd\"]:\n",
    "                    pred_aggettivo = child.text\n",
    "\n",
    "            if soggetto and oggetto:\n",
    "                relazione = {\"soggetto\": soggetto, \"verbo\": token.text, \"oggetto\": oggetto}\n",
    "                if relazione not in relazioni_svo:\n",
    "                    relazioni_svo.append(relazione)\n",
    "            elif soggetto and pred_aggettivo:\n",
    "                relazione = {\"soggetto\": soggetto, \"verbo\": token.text, \"predicato\": pred_aggettivo}\n",
    "                if relazione not in relazioni_svo:\n",
    "                    relazioni_svo.append(relazione)\n",
    "\n",
    "    entita_nominate = [{\"label\": ent.text, \"type\": ent.label_} for ent in doc.ents]\n",
    "\n",
    "    return soggetti, predicati, oggetti, aggettivi, avverbi, sostantivi, entita_nominate, relazioni_svo\n",
    "\n",
    "\n",
    "# Funzione per arricchire ogni frase e popolare Neo4j\n",
    "def enrich_sentence(row):\n",
    "    sentence = row['Frase']\n",
    "    soggetti, predicati, oggetti, aggettivi, avverbi, sostantivi, entita_nominate, relazioni_svo = extract_syntactic_roles_and_ner(sentence)\n",
    "\n",
    "    processed_terms = set()  # Cache per evitare cicli\n",
    "    populate_neo4j(sentence, soggetti, predicati, oggetti, aggettivi, avverbi, sostantivi, entita_nominate, relazioni_svo)\n",
    "    \n",
    "    return pd.Series({\n",
    "        'Soggetti': soggetti,\n",
    "        'Predicati': predicati,\n",
    "        'Oggetti': oggetti,\n",
    "        'Aggettivi': aggettivi,\n",
    "        'Avverbi': avverbi,\n",
    "        'Sostantivi': sostantivi,\n",
    "        'Entità Nominate': entita_nominate,\n",
    "        'Relazioni SVO': relazioni_svo\n",
    "    })\n",
    "\n",
    "\n",
    "def populate_neo4j(phrase, soggetti, predicati, oggetti, aggettivi, avverbi, sostantivi, entita_nominate, relazioni_svo):\n",
    "    print(f\"Inizio popolamento del database Neo4j per la frase: '{phrase}'\")\n",
    "\n",
    "    try:\n",
    "        # Inserimento delle entità nominate\n",
    "        for ent in entita_nominate:\n",
    "            process_entity(ent['label'], neo4j_handler)\n",
    "    \n",
    "        # Inserimento dei soggetti\n",
    "        for soggetto in soggetti:\n",
    "            roles = ['SUBJECT']\n",
    "            if soggetto in sostantivi:\n",
    "                roles.append('NOUN')\n",
    "            # Usa process_entity per ottenere descrizioni da Wikidata e WordNet\n",
    "            process_entity(soggetto, neo4j_handler)\n",
    "    \n",
    "        # Inserimento dei predicati\n",
    "        for predicato in predicati:\n",
    "            roles = ['PREDICATE']\n",
    "            process_verb_with_wordnet(neo4j_handler, predicato, roles)\n",
    "    \n",
    "        # Inserimento degli oggetti\n",
    "        for oggetto in oggetti:\n",
    "            roles = ['OBJECT']\n",
    "            if oggetto in sostantivi:\n",
    "                roles.append('NOUN')\n",
    "            # Usa process_entity per ottenere descrizioni da Wikidata e WordNet\n",
    "            process_entity(oggetto, neo4j_handler)\n",
    "    \n",
    "        # Inserimento degli aggettivi\n",
    "        for aggettivo in aggettivi:\n",
    "            # Usa process_entity per ottenere descrizioni da Wikidata e WordNet\n",
    "            process_entity(aggettivo, neo4j_handler)\n",
    "    \n",
    "        # Inserimento degli avverbi\n",
    "        for avverbio in avverbi:\n",
    "            # Usa process_entity per ottenere descrizioni da Wikidata e WordNet\n",
    "            process_entity(avverbio, neo4j_handler)\n",
    "    \n",
    "        # Inserimento delle relazioni SVO\n",
    "        for relazione in relazioni_svo:\n",
    "            soggetto = relazione.get('soggetto')\n",
    "            verbo = relazione.get('verbo')\n",
    "            oggetto = relazione.get('oggetto')\n",
    "            predicato = relazione.get('predicato')\n",
    "            \n",
    "            # Processiamo il soggetto\n",
    "            process_entity(soggetto, neo4j_handler)\n",
    "            process_verb_with_wordnet(neo4j_handler, verbo, ['PREDICATE'])\n",
    "            \n",
    "            # Se c'è un oggetto, processiamolo\n",
    "            if oggetto:\n",
    "                process_entity(oggetto, neo4j_handler)\n",
    "                neo4j_handler.create_relationship_if_not_exists(verbo, oggetto, \"APPLIES_TO\")\n",
    "            elif predicato:\n",
    "                process_entity(predicato, neo4j_handler)\n",
    "                neo4j_handler.create_relationship_if_not_exists(verbo, predicato, \"APPLIES_TO\")\n",
    "        \n",
    "    finally:\n",
    "        print(\"Popolamento del database completato\")\n",
    "\n",
    "try:\n",
    "    neo4j_handler = Neo4jHandler(\"bolt://localhost:7687\", \"neo4j\", \"10086832\")\n",
    "    \n",
    "    # Main logic to load dataset, enrich sentences, and populate Neo4j\n",
    "    if not os.path.exists('stereoset_enriched.json'):\n",
    "        print(\"Caricamento del modello Spacy per POS tagging, Dependency Parsing e NER...\")\n",
    "        if not 'nlp' in globals():\n",
    "            nlp = spacy.load(\"en_core_web_lg\")\n",
    "            print(\"Modello Spacy caricato.\")\n",
    "    \n",
    "        print(\"Caricamento del dataset Stereoset (intrasentence)...\")\n",
    "        intrasentence_dataset = load_dataset('McGill-NLP/stereoset', 'intrasentence')\n",
    "        print(\"Dataset Stereoset caricato.\")\n",
    "    \n",
    "        data = []\n",
    "        for item in intrasentence_dataset['validation']:\n",
    "            context = item['context']\n",
    "            target = item['target']\n",
    "            bias_type = item['bias_type']\n",
    "            \n",
    "            for i, sentence in enumerate(item['sentences']['sentence']):\n",
    "                gold_label = item['sentences']['gold_label'][i]  \n",
    "                data.append({\n",
    "                    'Contesto': context,\n",
    "                    'Frase': sentence,\n",
    "                    'Target': target,\n",
    "                    'Bias Type': bias_type,\n",
    "                    'Gold Label': gold_label\n",
    "                })\n",
    "    \n",
    "        df = pd.DataFrame(data)\n",
    "    \n",
    "        print(\"Estrazione dei ruoli sintattici, NER e relazioni SVO per tutte le frasi...\")\n",
    "        df_enriched = df.apply(lambda row: enrich_sentence(row), axis=1)\n",
    "        print(\"Estrazione completata.\")\n",
    "        \n",
    "        df_combined = pd.concat([df, df_enriched], axis=1)\n",
    "        df_combined.to_json('stereoset_enriched.json', orient='records', indent=4)\n",
    "        print(f\"Dataset arricchito con POS, NER, relazioni SVO salvato in 'stereoset_enriched.json'.\")\n",
    "    \n",
    "        neo4j_handler.close()\n",
    "    else:\n",
    "        print(f\"Caricamento del file JSON arricchito 'stereoset_enriched.json'\")\n",
    "        with open('stereoset_enriched.json', 'r') as stereoset_file:\n",
    "            df_final = json.load(stereoset_file)\n",
    "        print(\"Contenuto delle prime 5 righe del dataset Stereoset arricchito:\")\n",
    "        print(df_final[:5])\n",
    "except Exception as e:\n",
    "    print(f\"Errore di connessione a Neo4j: {e}\")\n",
    "    exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65f8137-cd33-43f9-a88c-cf38387527c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
