{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23656378-cb3f-4b55-ab78-76034b6daf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eseguo classificazione supervisionata...\n",
      "Pseudo-labels generate dal classificatore: [1 1 2 ... 0 0 2]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from umap import UMAP\n",
    "import spacy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Caricamento di Spacy per l'estrazione delle embedding e del testo distintivo\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "# Mappatura per Gold Label\n",
    "label_mapping = {\n",
    "    \"STEREOTYPED\": 0,\n",
    "    \"ANTISTEREOTYPICAL\": 1,\n",
    "    \"NEUTRAL\": 2\n",
    "}\n",
    "\n",
    "# Funzione per salvare i risultati dei cluster in formato JSON\n",
    "def save_clusters_to_json(documents, labels, filename, label_mapping):\n",
    "    clusters = defaultdict(list)\n",
    "    for doc, label in zip(documents, labels):\n",
    "        label_name = list(label_mapping.keys())[list(label_mapping.values()).index(label)]\n",
    "        clusters[label_name].append(doc)\n",
    "    \n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(clusters, file, indent=4)\n",
    "    print(f\"Risultati dei cluster salvati in {filename}\")\n",
    "\n",
    "# Funzione per visualizzare il clustering\n",
    "def plot_clusters(data, labels=None, title=\"Plot dei Cluster\"):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis', s=50, alpha=0.7)\n",
    "    plt.colorbar(label='Cluster Labels')\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Funzione per estrarre la porzione distintiva della frase\n",
    "def extract_distinctive_segment(phrase):\n",
    "    doc = nlp(phrase)\n",
    "    segments = [chunk.text for chunk in doc.noun_chunks if chunk.root.dep_ in {'nsubj', 'attr'}]\n",
    "    distinctive_part = \" \".join(segments[1:]) if len(segments) > 1 else phrase\n",
    "    return distinctive_part\n",
    "\n",
    "# Funzione per ottenere embedding basato sul testo distintivo\n",
    "def get_distinctive_embedding(phrase):\n",
    "    distinctive_part = extract_distinctive_segment(phrase)\n",
    "    distinctive_vector = nlp(distinctive_part).vector\n",
    "    return distinctive_vector\n",
    "\n",
    "# Funzione per aggregare i dati per Contesto\n",
    "def aggregate_by_context(data):\n",
    "    context_dict = defaultdict(lambda: {\"Bias Type\": None, \"Frasi\": []})\n",
    "    \n",
    "    for item in data:\n",
    "        context = item[\"Contesto\"]\n",
    "        bias_type = item[\"Bias Type\"]\n",
    "        \n",
    "        context_dict[context][\"Bias Type\"] = bias_type\n",
    "        context_dict[context][\"Frasi\"].append({\n",
    "            \"Frase\": item[\"Frase\"],\n",
    "            \"Gold Label\": item[\"Gold Label\"],\n",
    "            \"Frase completa\": item[\"Frase filtrata\"][\"Frase completa\"]\n",
    "        })\n",
    "    \n",
    "    aggregated_data = []\n",
    "    for context, values in context_dict.items():\n",
    "        labels = {frase[\"Gold Label\"] for frase in values[\"Frasi\"]}\n",
    "        if all(label in labels for label in label_mapping.keys()):\n",
    "            aggregated_data.append({\n",
    "                \"Contesto\": context,\n",
    "                \"Bias Type\": values[\"Bias Type\"],\n",
    "                \"Frasi\": values[\"Frasi\"]\n",
    "            })\n",
    "    \n",
    "    return aggregated_data\n",
    "\n",
    "# Funzione per bilanciare il set di allenamento per Bias Type e Contesto\n",
    "def create_balanced_training_set(aggregated_data, n_per_bias_type):\n",
    "    bias_type_groups = defaultdict(list)\n",
    "    for item in aggregated_data:\n",
    "        bias_type = item[\"Bias Type\"]\n",
    "        bias_type_groups[bias_type].append(item)\n",
    "    \n",
    "    training_set = []\n",
    "    test_set = []\n",
    "    for bias_type, contexts in bias_type_groups.items():\n",
    "        selected_contexts = random.sample(contexts, min(n_per_bias_type, len(contexts)))\n",
    "        training_set.extend(selected_contexts)\n",
    "        \n",
    "        remaining_contexts = [context for context in contexts if context not in selected_contexts]\n",
    "        test_set.extend(remaining_contexts)\n",
    "    \n",
    "    return training_set, test_set\n",
    "\n",
    "# Funzione per calcolare il numero massimo bilanciato di contesti per ciascun Bias Type\n",
    "def calculate_max_per_bias_type(aggregated_data):\n",
    "    bias_type_counts = defaultdict(int)\n",
    "    for item in aggregated_data:\n",
    "        bias_type_counts[item[\"Bias Type\"]] += 1\n",
    "    return min(bias_type_counts.values())\n",
    "\n",
    "# Caricamento del dataset\n",
    "with open('stereoset_with_multivectors_for_clustering.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Filtra i dati e recupera contesto, frase e multivettore\n",
    "filtered_data = [\n",
    "    item for item in data \n",
    "    if item[\"Frase filtrata\"][\"Frase completa\"][\"Tipo\"] == 2 and \"BLANK\" not in item[\"Frase filtrata\"][\"Frase completa\"][\"Testo\"]\n",
    "]\n",
    "\n",
    "# Aggrega i dati per Contesto\n",
    "aggregated_data = aggregate_by_context(filtered_data)\n",
    "\n",
    "# Calcola il numero massimo di contesti bilanciati per ciascun Bias Type\n",
    "n_per_bias_type = calculate_max_per_bias_type(aggregated_data)\n",
    "\n",
    "# Crea il dataset bilanciato per il training e il set di test\n",
    "training_data, test_data = create_balanced_training_set(aggregated_data, n_per_bias_type)\n",
    "\n",
    "# Estrai embedding distintive e label dal set di allenamento\n",
    "train_embeddings = np.array([\n",
    "    get_distinctive_embedding(frase[\"Frase completa\"][\"Testo\"])\n",
    "    for item in training_data for frase in item[\"Frasi\"]\n",
    "])\n",
    "train_labels = np.array([\n",
    "    label_mapping[frase[\"Gold Label\"]] for item in training_data for frase in item[\"Frasi\"]\n",
    "])\n",
    "\n",
    "# Estrai embedding distintive dal set di test\n",
    "test_documents = [frase[\"Frase completa\"][\"Testo\"] for item in test_data for frase in item[\"Frasi\"]]\n",
    "test_embeddings = np.array([\n",
    "    get_distinctive_embedding(frase[\"Frase completa\"][\"Testo\"])\n",
    "    for item in test_data for frase in item[\"Frasi\"]\n",
    "])\n",
    "\n",
    "# Esegui una classificazione supervisionata sui dati di training\n",
    "print(\"Eseguo classificazione supervisionata...\")\n",
    "classifier = SVC(kernel=\"linear\", probability=True, random_state=42)\n",
    "classifier.fit(train_embeddings, train_labels)\n",
    "\n",
    "# Classifica i dati di test\n",
    "pseudo_labels = classifier.predict(test_embeddings)\n",
    "print(\"Pseudo-labels generate dal classificatore:\", pseudo_labels)\n",
    "\n",
    "# Riduzione dimensionale per visualizzazione e clustering\n",
    "umap_reducer = UMAP(n_components=2, random_state=42, metric=\"cosine\")\n",
    "test_reduced = umap_reducer.fit_transform(test_embeddings)\n",
    "\n",
    "# Clustering con KMeans\n",
    "print(\"Eseguo clustering semi-supervisionato con KMeans...\")\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "test_cluster_labels_kmeans = kmeans.fit_predict(test_reduced)\n",
    "print(\"Etichette dei cluster KMeans:\", test_cluster_labels_kmeans)\n",
    "\n",
    "# Clustering con Agglomerative Clustering\n",
    "print(\"Eseguo clustering semi-supervisionato con Agglomerative Clustering...\")\n",
    "agg_clustering = AgglomerativeClustering(n_clusters=3, affinity='euclidean', linkage='ward')\n",
    "test_cluster_labels_agg = agg_clustering.fit_predict(test_reduced)\n",
    "print(\"Etichette dei cluster Agglomerative:\", test_cluster_labels_agg)\n",
    "\n",
    "# Visualizza e salva i risultati KMeans\n",
    "print(\"Visualizzo risultati KMeans\")\n",
    "plot_clusters(test_reduced, labels=test_cluster_labels_kmeans, title=\"Clustering KMeans su dati di test con pseudo-labels\")\n",
    "save_clusters_to_json(test_documents, test_cluster_labels_kmeans, \"cluster/kmeans_semi_supervised_test_results.json\", label_mapping)\n",
    "\n",
    "# Visualizza e salva i risultati Agglomerative\n",
    "print(\"Visualizzo risultati Agglomerative Clustering\")\n",
    "plot_clusters(test_reduced, labels=test_cluster_labels_agg, title=\"Clustering Agglomerative su dati di test con pseudo-labels\")\n",
    "save_clusters_to_json(test_documents, test_cluster_labels_agg, \"cluster/agglomerative_semi_supervised_test_results.json\", label_mapping)\n",
    "\n",
    "# Calcola silhouette score per valutare la separazione\n",
    "silhouette_avg_kmeans = silhouette_score(test_reduced, test_cluster_labels_kmeans)\n",
    "silhouette_avg_agg = silhouette_score(test_reduced, test_cluster_labels_agg)\n",
    "print(f\"Silhouette Score per KMeans: {silhouette_avg_kmeans:.4f}\")\n",
    "print(f\"Silhouette Score per Agglomerative Clustering: {silhouette_avg_agg:.4f}\")\n",
    "\n",
    "# Calcola l'accuratezza delle pseudo-etichetta per entrambi i metodi\n",
    "accuracy_kmeans = accuracy_score(pseudo_labels, test_cluster_labels_kmeans)\n",
    "accuracy_agg = accuracy_score(pseudo_labels, test_cluster_labels_agg)\n",
    "print(f\"Accuratezza delle pseudo-etichetta per KMeans: {accuracy_kmeans:.4f}\")\n",
    "print(f\"Accuratezza delle pseudo-etichetta per Agglomerative Clustering: {accuracy_agg:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7fa473-e0f6-47b2-bfd2-7decef7ba27d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
