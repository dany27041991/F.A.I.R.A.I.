{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dd7854-2792-4777-b1f6-6d967c1d8b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import string\n",
    "from scipy.cluster.hierarchy import cophenet\n",
    "from itertools import product\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import umap\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from scipy.spatial.distance import squareform\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "\n",
    "# Funzione per salvare i risultati dei cluster in formato JSON\n",
    "def save_clusters_to_json(documents, labels, filename):\n",
    "    clusters = defaultdict(list)\n",
    "    for doc, label in zip(documents, labels):\n",
    "        clusters[str(label)].append(doc)\n",
    "    \n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    with open(filename, 'w') as file:\n",
    "        json.dump(clusters, file, indent=4)\n",
    "    print(f\"Risultati dei cluster salvati in {filename}\")\n",
    "\n",
    "# Funzione per visualizzare il dendrogramma\n",
    "def plot_clusters(data, labels=None, title=\"Plot dei Cluster\", plot_type=\"scatter\", original_n_components=2):\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    \n",
    "    if plot_type == \"scatter\":\n",
    "        # Riduci a 2 dimensioni solo per il plotting se n_components > 2\n",
    "        if original_n_components > 2:\n",
    "            print(f\"Avviso: n_components è {original_n_components}, quindi riduco a 2 dimensioni per il plotting.\")\n",
    "            data = PCA(n_components=2).fit_transform(data)\n",
    "        \n",
    "        scatter = plt.scatter(data[:, 0], data[:, 1], c=labels, cmap='viridis', s=50, alpha=0.7)\n",
    "        plt.colorbar(scatter, label='Cluster Labels')\n",
    "        plt.xlabel(\"Component 1\")\n",
    "        plt.ylabel(\"Component 2\")\n",
    "    \n",
    "    elif plot_type == \"dendrogram\":\n",
    "        # Assume che 'data' sia la linkage matrix\n",
    "        dendrogram(data, truncate_mode='level', p=10)\n",
    "        plt.xlabel(\"Numero di campioni\")\n",
    "        plt.ylabel(\"Distanza\")\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"plot_type deve essere 'scatter' o 'dendrogram'\")\n",
    "    \n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Funzione per riduzione dimensionale con UMAP o t-SNE, con opzione di parallelizzazione\n",
    "def reduce_dimensions(data, method='umap', n_components=2, use_umap=True, parallelize=True):\n",
    "    if use_umap:\n",
    "        if parallelize:\n",
    "            reducer = umap.UMAP(n_components=n_components, verbose=0)  # Senza random_state per parallelismo\n",
    "        else:\n",
    "            reducer = umap.UMAP(n_components=n_components, random_state=42, verbose=0)  # Con random_state per riproducibilità\n",
    "    else:\n",
    "        reducer = TSNE(n_components=n_components, random_state=42)\n",
    "    \n",
    "    reduced_data = reducer.fit_transform(data)\n",
    "    print(f\"Riduzione dimensionale completata usando {'UMAP' if use_umap else 't-SNE'} con {'parallelizzazione' if parallelize else 'riproducibilità'}\")\n",
    "    return reduced_data\n",
    "\n",
    "# Funzione per il clustering KMeans con Calinski-Harabasz Index\n",
    "def kmeans_clustering(data, n_clusters):\n",
    "    print(\"Avvio del clustering KMeans...\")\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=2, verbose=0)\n",
    "    # Adatta il modello ai dati e predice le etichette\n",
    "    labels = kmeans.fit_predict(data)\n",
    "    # Calcola il coefficiente di silhouette\n",
    "    silhouette_avg = silhouette_score(data, labels)\n",
    "    # Calcola il Davies-Bouldin Index\n",
    "    davies_bouldin = davies_bouldin_score(data, labels)\n",
    "    # Calcola il Calinski-Harabasz Index\n",
    "    calinski_harabasz = calinski_harabasz_score(data, labels)\n",
    "    \n",
    "    print(\"Coefficiente di silhouette medio per KMeans:\", silhouette_avg)\n",
    "    print(\"Davies-Bouldin Index per KMeans:\", davies_bouldin)\n",
    "    print(\"Calinski-Harabasz Index per KMeans:\", calinski_harabasz)\n",
    "    \n",
    "    return labels, silhouette_avg, davies_bouldin, calinski_harabasz\n",
    "\n",
    "# Funzione per il clustering Agglomerativo con Calinski-Harabasz Index\n",
    "def agglomerative_clustering(data, n_clusters, method_clustering):\n",
    "    print(\"Avvio del clustering Agglomerativo con metodo di linkage:\", method_clustering)\n",
    "    agglomerative = AgglomerativeClustering(n_clusters=n_clusters, metric='euclidean', linkage=method_clustering)\n",
    "    # Predice le etichette di clustering\n",
    "    labels = agglomerative.fit_predict(data)\n",
    "    # Calcola il coefficiente di silhouette\n",
    "    silhouette_avg = silhouette_score(data, labels)\n",
    "    # Calcola il Davies-Bouldin Index\n",
    "    davies_bouldin = davies_bouldin_score(data, labels)\n",
    "    # Calcola il Calinski-Harabasz Index\n",
    "    calinski_harabasz = calinski_harabasz_score(data, labels)\n",
    "    \n",
    "    print(\"Coefficiente di silhouette medio per Agglomerativo:\", silhouette_avg)\n",
    "    print(\"Davies-Bouldin Index per Agglomerativo:\", davies_bouldin)\n",
    "    print(\"Calinski-Harabasz Index per Agglomerativo:\", calinski_harabasz)\n",
    "    \n",
    "    return labels, silhouette_avg, davies_bouldin, calinski_harabasz\n",
    "\n",
    "# Funzione per il clustering Gerarchico\n",
    "def hierarchical_clustering(data, n_clusters, method_clustering):\n",
    "    print(\"Avvio del clustering Gerarchico con metodo di linkage:\", method_clustering)\n",
    "    # Calcola la matrice di distanza coseno\n",
    "    cosine_distances = 1 - cosine_similarity(data)\n",
    "    cosine_distances = np.maximum(cosine_distances, 0)\n",
    "    # Converte la matrice di distanza quadrata in forma condensata\n",
    "    condensed_distances = squareform(cosine_distances, checks=False)\n",
    "    # Calcola il clustering gerarchico\n",
    "    linkage_matrix = linkage(condensed_distances, method=method_clustering)\n",
    "    # Genera le etichette di clustering\n",
    "    labels = fcluster(linkage_matrix, t=n_clusters, criterion='maxclust')\n",
    "    # Calcola il coefficiente di silhouette\n",
    "    silhouette_avg = silhouette_score(data, labels)\n",
    "    # Calcola il Cophenetic Correlation Coefficient\n",
    "    coph_corr, _ = cophenet(linkage_matrix, pdist(data))\n",
    "    # Calcola il Davies-Bouldin Index\n",
    "    davies_bouldin = davies_bouldin_score(data, labels)\n",
    "    \n",
    "    print(\"Coefficiente di silhouette medio per clustering Gerarchico:\", silhouette_avg)\n",
    "    print(\"Cophenetic Correlation Coefficient per clustering Gerarchico:\", coph_corr)\n",
    "    print(\"Davies-Bouldin Index per clustering Gerarchico:\", davies_bouldin)\n",
    "    \n",
    "    return labels, silhouette_avg, linkage_matrix, coph_corr, davies_bouldin\n",
    "\n",
    "# Funzione per il clustering con UMAP e HDBSCAN\n",
    "def hdbscan_clustering_with_umap(data, min_cluster_size=5, min_samples=None, n_components=2):\n",
    "    print(\"Avvio del clustering HDBSCAN con riduzione dimensionale UMAP...\")\n",
    "    # Riduzione dimensionale con UMAP\n",
    "    reduced_data = reduce_dimensions(data, method='umap', n_components=n_components, use_umap=True, parallelize=True)\n",
    "    \n",
    "    # Clustering HDBSCAN\n",
    "    hdbscan = HDBSCAN(min_cluster_size=min_cluster_size, min_samples=min_samples)\n",
    "    labels = hdbscan.fit_predict(reduced_data)\n",
    "    \n",
    "    # Rimuove i punti rumorosi per il calcolo delle metriche\n",
    "    core_samples = reduced_data[labels != -1]\n",
    "    core_labels = labels[labels != -1]\n",
    "    \n",
    "    # Calcola le metriche di clustering solo sui punti assegnati ai cluster\n",
    "    if len(set(core_labels)) > 1:\n",
    "        silhouette_avg = silhouette_score(core_samples, core_labels)\n",
    "        davies_bouldin = davies_bouldin_score(core_samples, core_labels)\n",
    "        calinski_harabasz = calinski_harabasz_score(core_samples, core_labels)\n",
    "    else:\n",
    "        silhouette_avg = davies_bouldin = calinski_harabasz = None\n",
    "        print(\"HDBSCAN ha generato solo un cluster (o rumore) - non calcolo le metriche di qualità.\")\n",
    "    \n",
    "    print(\"Coefficiente di silhouette medio per HDBSCAN:\", silhouette_avg)\n",
    "    print(\"Davies-Bouldin Index per HDBSCAN:\", davies_bouldin)\n",
    "    print(\"Calinski-Harabasz Index per HDBSCAN:\", calinski_harabasz)\n",
    "    \n",
    "    return labels, silhouette_avg, davies_bouldin, calinski_harabasz, reduced_data\n",
    "\n",
    "# Funzione aggiornata per trovare i parametri ottimali e tenere traccia del miglior punteggio per ogni metodo\n",
    "def find_optimal_clustering_params(documents, embeddings, param_grid):\n",
    "    for params in product(*param_grid.values()):\n",
    "        param_dict = dict(zip(param_grid.keys(), params))\n",
    "        print(f\"Provo i parametri: {param_dict}\")\n",
    "        \n",
    "        labels, score = clustering_with_dimensionality_reduction(\n",
    "            documents,\n",
    "            embeddings,\n",
    "            n_clusters=param_dict['n_clusters'],\n",
    "            method=param_dict['dimensionality_reduction_method'],\n",
    "            clustering_method=param_dict['clustering_method'],\n",
    "            n_components=param_dict['n_components'],\n",
    "            use_umap=param_dict['use_umap'],\n",
    "            method_clustering=param_dict['method_clustering'],\n",
    "            plot_results=False  # Disabilita il plotting durante il grid search\n",
    "        )\n",
    "        \n",
    "        # Aggiorna i migliori parametri e punteggi per il metodo di clustering corrente\n",
    "        clustering_method = param_dict['clustering_method']\n",
    "        if score is not None and score > best_params_per_method[clustering_method]['score']:\n",
    "            best_params_per_method[clustering_method]['score'] = score\n",
    "            best_params_per_method[clustering_method]['params'] = param_dict\n",
    "            print(f\"Aggiornato miglior punteggio per {clustering_method}: {score:.4f}\")\n",
    "\n",
    "    return best_params_per_method\n",
    "\n",
    "def clustering_with_dimensionality_reduction(documents, embeddings, n_clusters=None, method='umap', clustering_method='kmeans', n_components=2, use_umap=True, method_clustering='ward', min_cluster_size=5, min_samples=None, plot_results=True):\n",
    "    scaler = StandardScaler()\n",
    "    embeddings_scaled = scaler.fit_transform(embeddings)\n",
    "    reduced_data = reduce_dimensions(embeddings_scaled, method=method, n_components=n_components, use_umap=use_umap, parallelize=True)\n",
    "\n",
    "    if clustering_method == 'kmeans':\n",
    "        labels, score, davies_bouldin, calinski_harabasz = kmeans_clustering(reduced_data, n_clusters)\n",
    "        gc.collect()\n",
    "        if plot_results:\n",
    "            plot_clusters(reduced_data, labels=labels, title=\"Cluster Plot per KMeans\", plot_type=\"scatter\", original_n_components=n_components)\n",
    "        \n",
    "    elif clustering_method == 'agglomerative':\n",
    "        labels, score, davies_bouldin, calinski_harabasz = agglomerative_clustering(reduced_data, n_clusters, method_clustering)\n",
    "        gc.collect()\n",
    "        if plot_results:\n",
    "            plot_clusters(reduced_data, labels=labels, title=\"Cluster Plot per Agglomerative Clustering\", plot_type=\"scatter\", original_n_components=n_components)\n",
    "        \n",
    "    elif clustering_method == 'hierarchical':\n",
    "        labels, score, linkage_matrix, coph_corr, davies_bouldin = hierarchical_clustering(reduced_data, n_clusters, method_clustering)\n",
    "        gc.collect()\n",
    "        if plot_results:\n",
    "            plot_clusters(linkage_matrix, title=\"Dendrogramma del Clustering Gerarchico\", plot_type=\"dendrogram\")\n",
    "        \n",
    "    elif clustering_method == 'hdbscan':\n",
    "        # Call hdbscan_clustering_with_umap without `original_n_components`\n",
    "        labels, score, davies_bouldin, calinski_harabasz, reduced_data = hdbscan_clustering_with_umap(\n",
    "            embeddings_scaled, min_cluster_size=min_cluster_size, min_samples=min_samples, n_components=n_components\n",
    "        )\n",
    "        gc.collect()\n",
    "        if plot_results:\n",
    "            plot_clusters(reduced_data, labels=labels, title=\"Cluster Plot per HDBSCAN\", plot_type=\"scatter\", original_n_components=n_components)\n",
    "    \n",
    "    save_clusters_to_json(documents, labels, f\"cluster/{clustering_method}_cluster_results.json\")\n",
    "    return labels, score\n",
    "\n",
    "# Dizionario per memorizzare i migliori parametri e punteggi per ogni metodo di clustering\n",
    "best_params_per_method = {\n",
    "    'kmeans': {'params': None, 'score': -1},\n",
    "    'agglomerative': {'params': None, 'score': -1},\n",
    "    'hierarchical': {'params': None, 'score': -1},\n",
    "    'hdbscan': {'params': None, 'score': -1}\n",
    "}\n",
    "\n",
    "# Modifica il grid search per includere HDBSCAN\n",
    "param_grid = {\n",
    "    'dimensionality_reduction_method': ['umap', 'tsne'],\n",
    "    #'clustering_method': ['agglomerative', 'hierarchical', 'hdbscan', 'kmeans'],\n",
    "    'clustering_method': ['hdbscan'],\n",
    "    'method_clustering': ['ward', 'average', 'single' ,'complete'],\n",
    "    'n_components': [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15],\n",
    "    'n_clusters': [3,5,7,10,13,15,17,20,23,25,27,30,33,35,37,40,43,45,47,50,52,55,57,60,62,65,67,70,72,75,77,80,82,85,87,90,92,95,97,100],\n",
    "    #'min_cluster_size': [5, 10, 15, 20, 25],  # Aggiunto per HDBSCAN\n",
    "    #'min_samples': [None, 1, 5, 10, 15],  # Aggiunto per HDBSCAN\n",
    "    #'use_umap': [True, False],\n",
    "    'use_umap': [True]\n",
    "}\n",
    "\n",
    "# Parametri di ricerca su griglia\n",
    "'''param_grid = {\n",
    "    'dimensionality_reduction_method': ['umap', 'tsne'],\n",
    "    'clustering_method': ['agglomerative'],\n",
    "    'method_clustering': ['ward', 'average', 'single', 'complete'],\n",
    "    'n_components': [2, 5, 7, 10, 13, 15, 17, 20],\n",
    "    'n_clusters': [5, 10, 15, 20, 25, 30, 35, 40, 45, 50, 55, 60, 65, 70, 75, 80, 85, 90, 95, 100],\n",
    "    'min_cluster_size': [5, 10, 15, 20, 25],  # Aggiunto per HDBSCAN\n",
    "    'min_samples': [None, 1, 5, 10, 15],  # Aggiunto per HDBSCAN\n",
    "    'use_umap': [True]\n",
    "}'''\n",
    "\n",
    "# Caricamento del dataset\n",
    "with open('stereoset_with_multivectors_for_clustering.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "type = 2\n",
    "\n",
    "# Filtra i dati in base al tipo scelto e alla presenza della stringa \"BLANK\" nel testo\n",
    "filtered_data = [\n",
    "    item for item in data \n",
    "    if item[\"Frase filtrata\"][\"Frase completa\"][\"Tipo\"] == type and \"BLANK\" not in item[\"Frase filtrata\"][\"Frase completa\"][\"Testo\"]\n",
    "]\n",
    "\n",
    "# Estrazione del \"Multivettore\" e del \"Testo\" per ogni elemento filtrato\n",
    "documents = [item[\"Frase filtrata\"][\"Frase completa\"][\"Testo\"] for item in filtered_data]\n",
    "multivectors = np.array([item[\"Frase filtrata\"][\"Frase completa\"][\"Multivettore\"] for item in filtered_data])\n",
    "\n",
    "# Ricerca dei parametri ottimali\n",
    "print(\"Ricerca dei parametri ottimali per il clustering...\")\n",
    "best_params_per_method = find_optimal_clustering_params(documents, multivectors, param_grid)\n",
    "\n",
    "# Clustering finale per ogni metodo utilizzando i migliori parametri trovati\n",
    "for method, best in best_params_per_method.items():\n",
    "    if best['params'] is not None:  # Verifica che esista un set di parametri validi per il metodo\n",
    "        print(f\"\\nEseguo il clustering {method} con i parametri ottimali...\")\n",
    "        labels, score = clustering_with_dimensionality_reduction(\n",
    "            documents,\n",
    "            multivectors,\n",
    "            n_clusters=best['params']['n_clusters'],\n",
    "            method=best['params']['dimensionality_reduction_method'],\n",
    "            clustering_method=method,\n",
    "            n_components=best['params']['n_components'],\n",
    "            use_umap=best['params']['use_umap'],\n",
    "            plot_results=True  # Abilita il plotting per il clustering finale\n",
    "        )\n",
    "        print(f\"Clustering {method} completato con Silhouette Score: {score:.4f}\")\n",
    "        print(f\"Migliori parametri: {best['params']}\")\n",
    "        print(\"***************************************************\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2fdb0c-8408-4ec9-9d73-26fa4c555b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
