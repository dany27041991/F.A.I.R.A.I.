{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98e5e870-3d99-41ce-afbc-9707314979e3",
   "metadata": {},
   "source": [
    "# STEREOSET\n",
    "**StereoSet** è un dataset progettato per misurare e rilevare i **bias nei modelli di linguaggio** rispetto a diverse categorie, come **genere, razza, religione, e orientamento sessuale**. Il dataset è suddiviso in **tre diverse sezioni** — **Intrasentence**, **Intersentence**, e **Commonsense** — che servono a valutare i bias in contesti differenti e con diversi tipi di frasi.\n",
    "\n",
    "### Le sezioni di StereoSet:\n",
    "\n",
    "#### 1. **Intrasentence**:\n",
    "- **Descrizione**: Questa sezione contiene frasi singole in cui il bias si trova **all'interno della frase stessa**. L'obiettivo è valutare se i modelli di linguaggio **preferiscono stereotipi** o frasi che trasmettono bias all'interno di una singola frase, rispetto a opzioni neutre o contrarie.\n",
    "- **Struttura**: Ogni esempio contiene una frase \"context\" seguita da tre possibili completamenti:\n",
    "  1. **Stereotypical (stereotipico)**: Una continuazione che riflette un bias o stereotipo.\n",
    "  2. **Anti-stereotypical (anti-stereotipico)**: Una continuazione che contrasta lo stereotipo.\n",
    "  3. **Unrelated (non correlato)**: Una continuazione che non è rilevante o non si collega direttamente alla frase.\n",
    "\n",
    "**Esempio**:\n",
    "- **Contesto**: \"John loves fixing cars. He must be a...\"\n",
    "  - **Stereotypical completamento**: \"mechanic.\"\n",
    "  - **Anti-stereotypical completamento**: \"teacher.\"\n",
    "  - **Unrelated completamento**: \"banana.\"\n",
    "\n",
    "In questo esempio, il modello viene testato per vedere se predilige un completamento stereotipico, come \"mechanic\" (stereotipo di genere che associa gli uomini ai meccanici), rispetto a \"teacher\" o \"banana\".\n",
    "\n",
    "#### 2. **Intersentence**:\n",
    "- **Descrizione**: Questa sezione contiene **due frasi consecutive**, in cui il bias è presente **nella relazione tra le due frasi**. Qui, il bias può essere testato rispetto al modo in cui il modello collega il contesto di una frase alla successiva.\n",
    "- **Struttura**: Come in Intrasentence, anche qui ci sono tre opzioni per la seconda frase:\n",
    "  1. **Stereotypical**: Una seconda frase che segue uno stereotipo.\n",
    "  2. **Anti-stereotypical**: Una seconda frase che contrasta lo stereotipo.\n",
    "  3. **Unrelated**: Una seconda frase non collegata.\n",
    "\n",
    "**Esempio**:\n",
    "- **Frase 1**: \"Jane works as a software engineer.\"\n",
    "  - **Stereotypical seconda frase**: \"She must be bad at coding.\"\n",
    "  - **Anti-stereotypical seconda frase**: \"She is great at coding.\"\n",
    "  - **Unrelated seconda frase**: \"She loves painting on weekends.\"\n",
    "\n",
    "In questo caso, il modello è testato per verificare se associa in modo stereotipato una donna ingegnere a una valutazione negativa delle sue capacità tecniche.\n",
    "\n",
    "#### 3. **Commonsense**:\n",
    "- **Descrizione**: Questa sezione valuta le conoscenze generali e il ragionamento **basato sul buon senso**, senza necessariamente includere stereotipi. Questo serve a garantire che il modello non solo eviti i bias, ma che abbia anche una buona comprensione del **buon senso**.\n",
    "- **Struttura**: Ogni esempio commonsense presenta una frase e tre possibili completamenti. Qui, l'obiettivo è vedere se il modello può scegliere il completamento che ha più senso a livello logico.\n",
    "  1. **True**: Un completamento che ha senso dal punto di vista del buon senso.\n",
    "  2. **False**: Un completamento che non ha senso.\n",
    "  3. **Unrelated**: Un completamento che non è rilevante per la frase.\n",
    "\n",
    "**Esempio**:\n",
    "- **Frase**: \"The sun rises in the...\"\n",
    "  - **True completamento**: \"east.\"\n",
    "  - **False completamento**: \"west.\"\n",
    "  - **Unrelated completamento**: \"ocean.\"\n",
    "\n",
    "In questo caso, il modello deve dimostrare di avere un buon livello di conoscenza del mondo reale, scegliendo la risposta corretta (\"east\") rispetto a risposte errate o non correlate.\n",
    "\n",
    "### Obiettivo delle tre sezioni\n",
    "\n",
    "- **Intrasentence** e **Intersentence** sono progettate per rilevare **bias e stereotipi** nelle preferenze del modello, testando se è incline a scegliere frasi stereotipate rispetto a frasi anti-stereotipate o neutrali.\n",
    "- **Commonsense** serve come controllo per garantire che un modello non stia solo evitando bias, ma che possieda anche una buona comprensione del **ragionamento basato sul buon senso**. Questo è importante perché un modello che evita i bias, ma non ha un buon ragionamento, non sarà utile in molte applicazioni pratiche.\n",
    "\n",
    "### Dettagli sui tipi di bias rilevati\n",
    "\n",
    "StereoSet classifica i bias in **quattro categorie principali**:\n",
    "1. **Genere**: Bias legati a ruoli o caratteristiche di genere (es. uomini associati a lavori tecnici, donne a ruoli di cura).\n",
    "2. **Razza**: Bias basati su stereotipi razziali (es. associando determinate capacità o comportamenti a un gruppo etnico specifico).\n",
    "3. **Religione**: Bias riguardanti stereotipi religiosi (es. associando determinate credenze o comportamenti a una religione).\n",
    "4. **Orientamento sessuale**: Bias verso persone di orientamenti sessuali diversi (es. stereotipi sull'omosessualità o l'eterosessualità).\n",
    "\n",
    "### Utilizzo del dataset\n",
    "\n",
    "Puoi usare il dataset per **valutare i modelli di linguaggio** e verificare se tendono a favorire risposte stereotipate o se mostrano segni di bias nei loro completamenti di frasi. La struttura del dataset ti permette di confrontare le risposte stereotipate con quelle anti-stereotipate e di misurare il livello di bias presente nel modello."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "423d50e8-c05f-4959-9a7e-5416f29bae9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/danilogiovannico/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/danilogiovannico/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/danilogiovannico/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "import numpy as np\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "from neo4j import GraphDatabase\n",
    "from nltk.corpus import wordnet as wn\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Scarica il corpus WordNet\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "# Assicuriamoci di scaricare le stopwords di NLTK (solo la prima volta)\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e7cd99e-e043-45fb-94d7-db38c69d3489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File di cache\n",
    "CACHE_FILE_ENTITIES = \"cache/entity_cache_populate_neo4j.json\"\n",
    "CACHE_FILE_PHRASES = \"cache/phrase_cache_populate_neo4j.json\"\n",
    "\n",
    "# Funzione per caricare la cache dal disco\n",
    "def load_cache(CACHE_FILE):\n",
    "    if os.path.exists(CACHE_FILE):\n",
    "        with open(CACHE_FILE, 'r') as cache_file:\n",
    "            return json.load(cache_file)\n",
    "    return {}\n",
    "\n",
    "# Funzione per salvare la cache sul disco\n",
    "def save_cache(CACHE_FILE, cache):\n",
    "    with open(CACHE_FILE, 'w') as cache_file:\n",
    "        json.dump(cache, cache_file, indent=4)\n",
    "\n",
    "# Carica la cache prima di popolare Neo4j\n",
    "entity_cache = load_cache(CACHE_FILE_ENTITIES)\n",
    "phrase_cache = load_cache(CACHE_FILE_PHRASES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b279add-4d43-4c71-847c-47bec750b07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stopwords_and_lemmatizer(text, remove_stopwords=True, lemmatize=True):\n",
    "    # Converti la frase in un oggetto SpaCy per l'analisi\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    # Rimuove le stopwords e/o lemmatizza in base ai parametri\n",
    "    processed_tokens = []\n",
    "    for token in doc:\n",
    "        if remove_stopwords and token.text.lower() in stop_words:\n",
    "            continue\n",
    "        if lemmatize:\n",
    "            processed_tokens.append(token.lemma_)\n",
    "        else:\n",
    "            processed_tokens.append(token.text)\n",
    "    \n",
    "    # Unisce la frase pre-elaborata\n",
    "    return \" \".join(processed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d65f8137-cd33-43f9-a88c-cf38387527c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configura la connessione a Neo4j\n",
    "class Neo4jHandler:\n",
    "    def __init__(self, uri, user, password):\n",
    "        self.driver = GraphDatabase.driver(uri, auth=(user, password))\n",
    "\n",
    "    def close(self):\n",
    "        self.driver.close()\n",
    "\n",
    "    def create_node_if_not_exists(self, term, roles, wikidata_description=None, wordnet_description=None):\n",
    "        \"\"\"\n",
    "        Crea un nodo se non esiste già. Usa il termine come ID del nodo.\n",
    "        Se il nodo esiste già, aggiorna la sua descrizione se disponibile una nuova da Wikidata o WordNet.\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            # Concateniamo le descrizioni se disponibili senza aggiungere prefissi\n",
    "            descriptions = []\n",
    "            if wikidata_description:\n",
    "                descriptions.append(wikidata_description)  # Descrizione da Wikidata\n",
    "            if wordnet_description:\n",
    "                descriptions.append(wordnet_description)  # Descrizione da WordNet\n",
    "            final_description = \" | \".join(descriptions) if descriptions else \"\"\n",
    "    \n",
    "            # Primo step: creiamo o aggiorniamo il nodo. Se esiste, aggiorniamo la descrizione.\n",
    "            query_merge = \"\"\"\n",
    "            MERGE (n:Entity {id: $term})\n",
    "            ON CREATE SET n.label = $label, n.description = $description\n",
    "            ON MATCH SET n.description = CASE \n",
    "                WHEN $description <> '' THEN $description \n",
    "                ELSE n.description \n",
    "            END\n",
    "            RETURN n\n",
    "            \"\"\"\n",
    "            session.run(query_merge, term=term, label='|'.join(roles), description=final_description)\n",
    "    \n",
    "            # Secondo step: aggiorniamo la label solo se non è già inclusa\n",
    "            for role in roles:\n",
    "                query_update_label = \"\"\"\n",
    "                MATCH (n:Entity {id: $term})\n",
    "                WHERE NOT n.label CONTAINS $role\n",
    "                SET n.label = n.label + '|' + $role\n",
    "                RETURN n\n",
    "                \"\"\"\n",
    "                session.run(query_update_label, term=term, role=role)\n",
    "\n",
    "    def create_relationship_if_not_exists(self, term_id, related_entity_id, relationship_type):\n",
    "        \"\"\"\n",
    "        Crea una relazione tra due nodi solo se non esiste già.\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            query = f\"\"\"\n",
    "            MATCH (a:Entity {{id: $term_id}}), (b:Entity {{id: $related_entity_id}})\n",
    "            MERGE (a)-[r:{relationship_type}]->(b)\n",
    "            RETURN r\n",
    "            \"\"\"\n",
    "            session.run(query, term_id=term_id, related_entity_id=related_entity_id)\n",
    "\n",
    "    def verb_exists_in_neo4j(self, verbo):\n",
    "        \"\"\"\n",
    "        Controlla se un nodo per il verbo (lemma) esiste già nel grafo Neo4j.\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            query = \"\"\"\n",
    "            MATCH (n:Entity {id: $verbo})\n",
    "            RETURN n LIMIT 1\n",
    "            \"\"\"\n",
    "            result = session.run(query, verbo=verbo)\n",
    "            return result.single() is not None\n",
    "\n",
    "    def get_entity_info_from_neo4j(self, term):\n",
    "        query = \"\"\"\n",
    "        MATCH (n:Entity {id: $term})\n",
    "        OPTIONAL MATCH (n)-[:INSTANCE_OF]->(instance)\n",
    "        OPTIONAL MATCH (n)-[:SUBCLASS_OF]->(subclass)\n",
    "        RETURN n.description AS description, collect(DISTINCT instance.id) AS instance_of, collect(DISTINCT subclass.id) AS subclass_of\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(query, term=term).single()\n",
    "        \n",
    "        return {\n",
    "            'description': result['description'] if result and result['description'] else [],\n",
    "            'instance_of': result['instance_of'] if result and result['instance_of'] else [],\n",
    "            'subclass_of': result['subclass_of'] if result and result['subclass_of'] else []\n",
    "        }\n",
    "\n",
    "\n",
    "    def get_verb_info_from_neo4j(self, term):\n",
    "        query = \"\"\"\n",
    "        MATCH (n:Entity {id: $term})\n",
    "        OPTIONAL MATCH (n)-[:SYNONYM_OF]->(synonym)\n",
    "        OPTIONAL MATCH (n)-[:ANTONYM_OF]->(antonym)\n",
    "        RETURN n.description AS description, collect(DISTINCT synonym.id) AS synonyms, collect(DISTINCT antonym.id) AS antonyms\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(query, term=term).single()\n",
    "        \n",
    "        return {\n",
    "            'description': result['description'] if result and result['description'] else [],\n",
    "            'synonyms': result['synonyms'] if result and result['synonyms'] else [],\n",
    "            'antonyms': result['antonyms'] if result and result['antonyms'] else []\n",
    "        }\n",
    "\n",
    "    def get_adj_adv_info_from_neo4j(self, term):\n",
    "        query = \"\"\"\n",
    "        MATCH (n:Entity {id: $term})\n",
    "        OPTIONAL MATCH (n)-[:SYNONYM_OF]->(synonym)\n",
    "        OPTIONAL MATCH (n)-[:ANTONYM_OF]->(antonym)\n",
    "        RETURN collect(DISTINCT synonym.id) AS synonyms, collect(DISTINCT antonym.id) AS antonyms\n",
    "        \"\"\"\n",
    "        with self.driver.session() as session:\n",
    "            result = session.run(query, term=term).single()\n",
    "        return {\n",
    "            'synonyms': result['synonyms'],\n",
    "            'antonyms': result['antonyms']\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c571ce5c-73cf-412a-81c0-aabf1511b22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "WIKIDATA_API_URL = \"https://www.wikidata.org/w/api.php\"\n",
    "\n",
    "def search_wikidata(term, limit=20, sleep=0):\n",
    "    \"\"\"\n",
    "    Cerca entità su Wikidata e ritorna id, label e descrizione.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"action\": \"wbsearchentities\",\n",
    "        \"search\": term,\n",
    "        \"language\": \"en\",\n",
    "        \"format\": \"json\",\n",
    "        \"limit\": limit\n",
    "    }\n",
    "    time.sleep(sleep)\n",
    "    response = requests.get(WIKIDATA_API_URL, params=params)\n",
    "    results = response.json().get(\"search\", [])\n",
    "    \n",
    "    entities = []\n",
    "    for entity in results:\n",
    "        entities.append({\n",
    "            \"id\": entity.get(\"id\"),\n",
    "            \"label\": entity.get(\"label\"),\n",
    "            \"description\": entity.get(\"description\")\n",
    "        })\n",
    "    \n",
    "    return entities\n",
    "\n",
    "def get_entity_data(entity_id, sleep=0.1):\n",
    "    \"\"\"\n",
    "    Ottiene le proprietà 'instance of' e 'subclass of' per una determinata entità.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"action\": \"wbgetentities\",\n",
    "        \"ids\": entity_id,\n",
    "        \"props\": \"claims\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    time.sleep(sleep)\n",
    "    response = requests.get(WIKIDATA_API_URL, params=params)\n",
    "    entity_data = response.json().get(\"entities\", {}).get(entity_id, {})\n",
    "    claims = entity_data.get(\"claims\", {})\n",
    "    \n",
    "    instance_of = claims.get(\"P31\", [])\n",
    "    subclass_of = claims.get(\"P279\", [])\n",
    "    \n",
    "    return instance_of, subclass_of\n",
    "\n",
    "def extract_labels_descriptions(property_claims, sleep=0):\n",
    "    \"\"\"\n",
    "    Estrae etichette e descrizioni per ogni entità associata a 'instance of' o 'subclass of'.\n",
    "    \"\"\"\n",
    "    ids = [claim[\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"] for claim in property_claims if \"mainsnak\" in claim and \"datavalue\" in claim[\"mainsnak\"]]\n",
    "    \n",
    "    if not ids:\n",
    "        return []\n",
    "    \n",
    "    ids_str = \"|\".join(ids)\n",
    "    \n",
    "    params = {\n",
    "        \"action\": \"wbgetentities\",\n",
    "        \"ids\": ids_str,\n",
    "        \"props\": \"labels|descriptions\",\n",
    "        \"languages\": \"en\",  # Otteniamo etichette e descrizioni in inglese\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    time.sleep(sleep)\n",
    "    response = requests.get(WIKIDATA_API_URL, params=params)\n",
    "    entities = response.json().get(\"entities\", {})\n",
    "    \n",
    "    results = []\n",
    "    for entity_id, entity_info in entities.items():\n",
    "        # Estrai la descrizione dall'entità, se disponibile\n",
    "        label = entity_info.get(\"labels\", {}).get(\"en\", {}).get(\"value\", \"\")\n",
    "        description = entity_info.get(\"descriptions\", {}).get(\"en\", {}).get(\"value\", \"\")\n",
    "        \n",
    "        # Se non c'è descrizione, imposta un valore predefinito o lascialo vuoto\n",
    "        description = description if description else \"\"\n",
    "                \n",
    "        results.append({\n",
    "            \"id\": entity_id,\n",
    "            \"label\": label,\n",
    "            \"description\": description  # Usa la descrizione estratta da Wikidata\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "def save_to_neo4j(neo4j_handler, term, instance_of, subclass_of, wordnet_description=None):\n",
    "    \"\"\"\n",
    "    Salva i dati estratti su Neo4j e crea relazioni 'instance of' e 'subclass of'.\n",
    "    Usa descrizioni da Wikidata e WordNet, se disponibili.\n",
    "    \"\"\"\n",
    "    # Ottieni la descrizione da Wikidata se disponibile\n",
    "    wikidata_description = None\n",
    "    if instance_of:\n",
    "        wikidata_description = instance_of[0][\"description\"]  # Usa direttamente la descrizione da Wikidata\n",
    "    elif subclass_of:\n",
    "        wikidata_description = subclass_of[0][\"description\"]  # Usa direttamente la descrizione da Wikidata\n",
    "\n",
    "    # Se esiste una descrizione da Wikidata, usiamola\n",
    "    # Aggiungiamo anche la descrizione da WordNet se disponibile\n",
    "    final_description = \"\"\n",
    "    if wikidata_description and wikidata_description != \"No description available\":\n",
    "        final_description += wikidata_description\n",
    "    if wordnet_description:\n",
    "        if final_description:\n",
    "            final_description += \" | \"  # Concatena le descrizioni se entrambe esistono\n",
    "        final_description += wordnet_description\n",
    "\n",
    "    # Passiamo la descrizione combinata (senza prefissi) al nodo\n",
    "    neo4j_handler.create_node_if_not_exists(term, ['ENTITY'], final_description)\n",
    "    \n",
    "    # Inserisci i nodi e le relazioni 'instance of'\n",
    "    for instance in instance_of:\n",
    "        neo4j_handler.create_node_if_not_exists(instance[\"label\"], ['INSTANCE'], instance[\"description\"])\n",
    "        neo4j_handler.create_relationship_if_not_exists(term, instance[\"label\"], \"INSTANCE_OF\")\n",
    "    \n",
    "    # Inserisci i nodi e le relazioni 'subclass of'\n",
    "    for subclass in subclass_of:\n",
    "        neo4j_handler.create_node_if_not_exists(subclass[\"label\"], ['SUBCLASS'], subclass[\"description\"])\n",
    "        neo4j_handler.create_relationship_if_not_exists(term, subclass[\"label\"], \"SUBCLASS_OF\")\n",
    "\n",
    "def enrich_with_synonyms_and_antonyms(neo4j_handler, term):\n",
    "    \"\"\"\n",
    "    Arricchisce il nodo principale con tutti i sinonimi e contrari trovati tramite WordNet.\n",
    "    I sinonimi e contrari vengono tutti collegati direttamente al nodo principale.\n",
    "    \"\"\"\n",
    "    synsets = wn.synsets(term)\n",
    "\n",
    "    if not synsets:\n",
    "        print(f\"Nessun synset trovato per il termine '{term}'\")\n",
    "        return\n",
    "\n",
    "    processed_synonyms = set()  # Cache per tenere traccia dei sinonimi già elaborati\n",
    "\n",
    "    for synset in synsets:\n",
    "        lemmas = synset.lemmas()\n",
    "        \n",
    "        for lemma in lemmas:\n",
    "            synonym = lemma.name()\n",
    "            \n",
    "            # Assicuriamoci di non creare duplicati\n",
    "            if synonym != term and synonym not in processed_synonyms:\n",
    "                processed_synonyms.add(synonym)\n",
    "                # Ottieni la definizione di WordNet per il sinonimo\n",
    "                description = synset.definition() if synset else f\"Synonym of {term}\"\n",
    "                #print(f\"Processo sinonimo: {synonym}\")\n",
    "                # Crea il nodo per il sinonimo SENZA label\n",
    "                neo4j_handler.create_node_if_not_exists(synonym, [], description)\n",
    "                neo4j_handler.create_relationship_if_not_exists(term, synonym, \"SYNONYM_OF\")\n",
    "            \n",
    "            antonyms = lemma.antonyms()\n",
    "            \n",
    "            for antonym in antonyms:\n",
    "                antonym_name = antonym.name()\n",
    "                if antonym_name != term and antonym_name not in processed_synonyms:\n",
    "                    processed_synonyms.add(antonym_name)\n",
    "                    # Ottieni la definizione di WordNet per il contrario\n",
    "                    antonym_synset = wn.synsets(antonym_name)\n",
    "                    description = antonym_synset[0].definition() if antonym_synset else f\"Antonym of {term}\"\n",
    "                    #print(f\"Processo antonimo: {antonym_name}\")\n",
    "                    # Crea il nodo per il contrario SENZA label\n",
    "                    neo4j_handler.create_node_if_not_exists(antonym_name, [], description)\n",
    "                    neo4j_handler.create_relationship_if_not_exists(term, antonym_name, \"ANTONYM_OF\")\n",
    "\n",
    "def process_verb_with_wordnet(neo4j_handler, verbo, roles):\n",
    "    \"\"\"\n",
    "    Elabora i verbi (lemmi) arricchendoli con sinonimi e contrari da WordNet.\n",
    "    Gestisce anche le relazioni se il verbo esiste già.\n",
    "    \"\"\"\n",
    "    synsets = wn.synsets(verbo, pos=wn.VERB)\n",
    "    description = synsets[0].definition() if synsets else f\"Verb: {verbo}\"\n",
    "\n",
    "    # Controlliamo se il verbo esiste già\n",
    "    if neo4j_handler.verb_exists_in_neo4j(verbo):\n",
    "        #print(f\"Verbo '{verbo}' già presente nel grafo, creando comunque le relazioni.\")\n",
    "        # Anche se il nodo esiste, creiamo le relazioni e aggiorniamo i ruoli\n",
    "        neo4j_handler.create_node_if_not_exists(verbo, roles, description)\n",
    "        enrich_with_synonyms_and_antonyms(neo4j_handler, verbo)\n",
    "        return\n",
    "\n",
    "    # Aggiungi il verbo come nodo al grafo\n",
    "    neo4j_handler.create_node_if_not_exists(verbo, roles, description)\n",
    "    \n",
    "    # Aggiungi sinonimi e contrari per il verbo\n",
    "    enrich_with_synonyms_and_antonyms(neo4j_handler, verbo)\n",
    "\n",
    "\n",
    "def process_entity(term, neo4j_handler, limit=50):\n",
    "    \"\"\"\n",
    "    Processa un'entità, arricchendola con dati da Wikidata, e aggiunge sinonimi e contrari da WordNet.\n",
    "    \"\"\"\n",
    "    # Ottieni i dati da Wikidata\n",
    "    entities = search_wikidata(term, limit=limit)\n",
    "    \n",
    "    if not entities:\n",
    "        print(f\"Nessun risultato trovato per il termine '{term}'\")\n",
    "        return\n",
    "    \n",
    "    instance_of_entities = []\n",
    "    subclass_of_entities = []\n",
    "\n",
    "    for entity_data in entities:\n",
    "        entity_id = entity_data[\"id\"]\n",
    "        instance_of_claims, subclass_of_claims = get_entity_data(entity_id)\n",
    "        instance_of_entities.extend(extract_labels_descriptions(instance_of_claims))\n",
    "        subclass_of_entities.extend(extract_labels_descriptions(subclass_of_claims))\n",
    "    \n",
    "    # Ottieni la definizione di WordNet se esiste\n",
    "    wordnet_synsets = wn.synsets(term)\n",
    "    wordnet_description = wordnet_synsets[0].definition() if wordnet_synsets else None\n",
    "\n",
    "    # Salva i dati su Neo4j con entrambe le descrizioni\n",
    "    save_to_neo4j(neo4j_handler, term, instance_of_entities, subclass_of_entities, wordnet_description)\n",
    "\n",
    "    # Aggiungi sinonimi e contrari da WordNet\n",
    "    enrich_with_synonyms_and_antonyms(neo4j_handler, term)\n",
    "\n",
    "# Funzione per estrarre ruoli sintattici, NER e relazioni SVO\n",
    "def extract_syntactic_roles_and_ner(text):\n",
    "    \"\"\"\n",
    "    Estrae soggetti, predicati, oggetti, aggettivi, avverbi, sostantivi, entità nominate e relazioni SVO da un testo.\n",
    "    \"\"\"\n",
    "    doc = nlp(text)\n",
    "    \n",
    "    soggetti = []\n",
    "    predicati = []\n",
    "    oggetti = []\n",
    "    aggettivi = []\n",
    "    avverbi = []\n",
    "    sostantivi = []\n",
    "    entita_nominate = []\n",
    "    relazioni_svo = []\n",
    "\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"nsubj\":\n",
    "            soggetti.append(token.text)\n",
    "        if token.pos_ == \"VERB\" or token.pos_ == \"AUX\":\n",
    "            predicati.append(token.text)\n",
    "        if token.dep_ == \"dobj\":\n",
    "            oggetti.append(token.text)\n",
    "        if token.pos_ == \"ADJ\":\n",
    "            aggettivi.append(token.text)\n",
    "        if token.pos_ == \"ADV\":\n",
    "            avverbi.append(token.text)\n",
    "        if token.pos_ == \"NOUN\":\n",
    "            sostantivi.append(token.text)\n",
    "\n",
    "    for token in doc:\n",
    "        if token.dep_ == \"ROOT\":\n",
    "            soggetto = None\n",
    "            oggetto = None\n",
    "            pred_aggettivo = None\n",
    "\n",
    "            for child in token.children:\n",
    "                if child.dep_ in [\"nsubj\", \"nsubjpass\", \"csubj\"]:\n",
    "                    soggetto = child.text\n",
    "\n",
    "            for child in token.children:\n",
    "                if child.dep_ in [\"dobj\", \"pobj\", \"iobj\"]:\n",
    "                    oggetto = child.text\n",
    "                if child.dep_ in [\"acomp\", \"attr\", \"oprd\"]:\n",
    "                    pred_aggettivo = child.text\n",
    "\n",
    "            if soggetto and oggetto:\n",
    "                relazione = {\"soggetto\": soggetto, \"verbo\": token.text, \"oggetto\": oggetto}\n",
    "                if relazione not in relazioni_svo:\n",
    "                    relazioni_svo.append(relazione)\n",
    "            elif soggetto and pred_aggettivo:\n",
    "                relazione = {\"soggetto\": soggetto, \"verbo\": token.text, \"predicato\": pred_aggettivo}\n",
    "                if relazione not in relazioni_svo:\n",
    "                    relazioni_svo.append(relazione)\n",
    "\n",
    "    entita_nominate = [{\"label\": ent.text, \"type\": ent.label_} for ent in doc.ents]\n",
    "\n",
    "    return soggetti, predicati, oggetti, aggettivi, avverbi, sostantivi, entita_nominate, relazioni_svo\n",
    "\n",
    "\n",
    "# Funzione per arricchire ogni frase e popolare Neo4j\n",
    "def enrich_sentence(row, enable_neo4j_population=True, enable_context=False):\n",
    "    sentence = row['Frase']\n",
    "    soggetti, predicati, oggetti, aggettivi, avverbi, sostantivi, entita_nominate, relazioni_svo = extract_syntactic_roles_and_ner(sentence)\n",
    "\n",
    "    if enable_neo4j_population == True:\n",
    "        populate_neo4j(sentence, soggetti, predicati, oggetti, aggettivi, avverbi, sostantivi, entita_nominate, relazioni_svo)\n",
    "        context = row['Contesto']\n",
    "        if 'BLANK' not in context:\n",
    "            soggetti_contesto, predicati_contesto, oggetti_contesto, aggettivi_contesto, avverbi_contesto, sostantivi_contesto, entita_nominate_contesto, relazioni_svo_contesto = extract_syntactic_roles_and_ner(context)\n",
    "            populate_neo4j(context, soggetti_contesto, predicati_contesto, oggetti_contesto, aggettivi_contesto, avverbi_contesto, sostantivi_contesto, entita_nominate_contesto, relazioni_svo_contesto)\n",
    "    \n",
    "    return pd.Series({\n",
    "        'Soggetti': soggetti,\n",
    "        'Predicati': predicati,\n",
    "        'Oggetti': oggetti,\n",
    "        'Aggettivi': aggettivi,\n",
    "        'Avverbi': avverbi,\n",
    "        'Sostantivi': sostantivi,\n",
    "        #'Entità Nominate': entita_nominate,\n",
    "        'Relazioni SVO': relazioni_svo\n",
    "    })\n",
    "\n",
    "\n",
    "def populate_neo4j(phrase, soggetti, predicati, oggetti, aggettivi, avverbi, sostantivi, entita_nominate, relazioni_svo):\n",
    "    if phrase not in phrase_cache:\n",
    "        print(f\"Inizio popolamento del database Neo4j per la frase: '{phrase}'\")\n",
    "        # Inserimento delle entità nominate\n",
    "        for ent in entita_nominate:\n",
    "            term = ent['label']\n",
    "            if term not in entity_cache:\n",
    "                process_entity(term, neo4j_handler)  # Usa il normale metodo process_entity\n",
    "                entity_cache[term] = True  # Memorizza l'entità nella cache\n",
    "                save_cache(CACHE_FILE_ENTITIES, entity_cache)  # Salva la cache su disco\n",
    "    \n",
    "        # Inserimento dei soggetti\n",
    "        for soggetto in soggetti:\n",
    "            roles = ['SUBJECT']\n",
    "            if soggetto in sostantivi:\n",
    "                roles.append('NOUN')\n",
    "            if soggetto not in entity_cache:\n",
    "                process_entity(soggetto, neo4j_handler)  # Usa il normale metodo process_entity\n",
    "                entity_cache[soggetto] = True  # Memorizza il soggetto nella cache\n",
    "                save_cache(CACHE_FILE_ENTITIES, entity_cache)  # Salva la cache su disco\n",
    "    \n",
    "        # Inserimento dei predicati\n",
    "        for predicato in predicati:\n",
    "            roles = ['PREDICATE']\n",
    "            if predicato not in entity_cache:\n",
    "                process_verb_with_wordnet(neo4j_handler, predicato, roles)  # Usa process_verb_with_wordnet\n",
    "                entity_cache[predicato] = True  # Memorizza il predicato nella cache\n",
    "                save_cache(CACHE_FILE_ENTITIES, entity_cache)  # Salva la cache su disco\n",
    "    \n",
    "        # Inserimento degli oggetti\n",
    "        for oggetto in oggetti:\n",
    "            roles = ['OBJECT']\n",
    "            if oggetto in sostantivi:\n",
    "                roles.append('NOUN')\n",
    "            if oggetto not in entity_cache:\n",
    "                process_entity(oggetto, neo4j_handler)  # Usa il normale metodo process_entity\n",
    "                entity_cache[oggetto] = True  # Memorizza l'oggetto nella cache\n",
    "                save_cache(CACHE_FILE_ENTITIES, entity_cache)  # Salva la cache su disco\n",
    "    \n",
    "        # Inserimento degli aggettivi\n",
    "        for aggettivo in aggettivi:\n",
    "            if aggettivo not in entity_cache:\n",
    "                process_entity(aggettivo, neo4j_handler)  # Usa il normale metodo process_entity\n",
    "                entity_cache[aggettivo] = True  # Memorizza l'aggettivo nella cache\n",
    "                save_cache(CACHE_FILE_ENTITIES, entity_cache)  # Salva la cache su disco\n",
    "    \n",
    "        # Inserimento degli avverbi\n",
    "        for avverbio in avverbi:\n",
    "            if avverbio not in entity_cache:\n",
    "                process_entity(avverbio, neo4j_handler)  # Usa il normale metodo process_entity\n",
    "                entity_cache[avverbio] = True  # Memorizza l'avverbio nella cache\n",
    "                save_cache(CACHE_FILE_ENTITIES, entity_cache)  # Salva la cache su disco\n",
    "    \n",
    "        # Inserimento delle relazioni SVO\n",
    "        for relazione in relazioni_svo:\n",
    "            soggetto = relazione.get('soggetto')\n",
    "            verbo = relazione.get('verbo')\n",
    "            oggetto = relazione.get('oggetto')\n",
    "            predicato = relazione.get('predicato')\n",
    "            # Processiamo il soggetto\n",
    "            process_entity(soggetto, neo4j_handler)\n",
    "            process_verb_with_wordnet(neo4j_handler, verbo, ['PREDICATE'])\n",
    "\n",
    "            # Creiamo la relazione tra soggetto e verbo (chi compie l'azione)\n",
    "            neo4j_handler.create_relationship_if_not_exists(soggetto, verbo, \"PERFORMS\")\n",
    "            \n",
    "            # Se c'è un oggetto, processiamolo\n",
    "            if oggetto:\n",
    "                process_entity(oggetto, neo4j_handler)\n",
    "                neo4j_handler.create_relationship_if_not_exists(verbo, oggetto, \"APPLIES_TO\")\n",
    "            elif predicato:\n",
    "                process_entity(predicato, neo4j_handler)\n",
    "                neo4j_handler.create_relationship_if_not_exists(verbo, predicato, \"APPLIES_TO\")\n",
    "\n",
    "        phrase_cache[phrase] = True\n",
    "        save_cache(CACHE_FILE_PHRASES, phrase_cache)  # Salva la cache su disco\n",
    "        print(\"Frase analizzata.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6e9f01e-945d-42d1-bbec-9509d4867d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sbert_embeddings(documents, use_batching=True):\n",
    "    if use_batching:\n",
    "        embeddings = sbert_model.encode(documents, convert_to_tensor=True)\n",
    "        return embeddings.cpu().numpy()  # Ritorna embeddings come numpy array\n",
    "    else:\n",
    "        return np.vstack([sbert_model.encode(doc) for doc in documents])\n",
    "\n",
    "def process_description_embeddings(descriptions, model):\n",
    "    \"\"\"Genera un embedding per ogni descrizione individualmente usando SentenceTransformer\"\"\"\n",
    "    \n",
    "    # Assicurati che descriptions sia una lista\n",
    "    if isinstance(descriptions, str):\n",
    "        descriptions = [descriptions]  # Se è una stringa, mettila in una lista\n",
    "    \n",
    "    # Controlla se la lista di descrizioni è vuota\n",
    "    if not descriptions:\n",
    "        return []  # Restituisce una lista vuota se non ci sono descrizioni\n",
    "    \n",
    "    # Genera embedding per tutte le descrizioni in batch\n",
    "    description_embeddings = generate_sbert_embeddings(descriptions)\n",
    "    return description_embeddings\n",
    "\n",
    "def create_multivector_with_svo_and_adj_adv(row, neo4j_handler, model, remove_stopwords=False, lemmatize=False):\n",
    "    context = row.get('Contesto')\n",
    "    sentence = row['Frase']\n",
    "    relazioni_svo = row.get('Relazioni SVO', [])\n",
    "    aggettivi = row.get('Aggettivi', [])\n",
    "    avverbi = row.get('Avverbi', [])\n",
    "\n",
    "    contextual_phrase = context + \" \" + sentence\n",
    "\n",
    "    # Embedding della frase\n",
    "    if remove_stopwords or lemmatize:\n",
    "        processed_text = remove_stopwords_and_lemmatizer(contextual_phrase, remove_stopwords=remove_stopwords, lemmatize=lemmatize)\n",
    "        contextual_embedding = generate_sbert_embeddings([processed_text])[0]\n",
    "    else:\n",
    "        contextual_embedding = generate_sbert_embeddings([contextual_phrase])[0]\n",
    "\n",
    "    enriched_embeddings = []\n",
    "    seen_embeddings = set()  # Set per tracciare gli embedding già aggiunti\n",
    "\n",
    "    def add_unique_embeddings(embeddings):\n",
    "        for embedding in embeddings:\n",
    "            embedding_tuple = tuple(embedding.flatten())  # Converte l'array in una tupla\n",
    "            if embedding_tuple not in seen_embeddings:\n",
    "                seen_embeddings.add(embedding_tuple)\n",
    "                enriched_embeddings.append(embedding)\n",
    "\n",
    "    # Processare le relazioni SVO con pre-processing su soggetto, verbo, oggetto, predicato\n",
    "    for relazione in relazioni_svo:\n",
    "        # Applicare il pre-processing indipendentemente dai flag per soggetto, verbo, oggetto, predicato\n",
    "        soggetto = remove_stopwords_and_lemmatizer(relazione.get('soggetto', ''), remove_stopwords=True, lemmatize=True)\n",
    "        verbo = remove_stopwords_and_lemmatizer(relazione.get('verbo', ''), remove_stopwords=True, lemmatize=True)\n",
    "        oggetto = remove_stopwords_and_lemmatizer(relazione.get('oggetto', ''), remove_stopwords=True, lemmatize=True) if relazione.get('oggetto') else ''\n",
    "        predicato = remove_stopwords_and_lemmatizer(relazione.get('predicato', ''), remove_stopwords=True, lemmatize=True) if relazione.get('predicato') else ''\n",
    "\n",
    "        # Ottenere informazioni sugli elementi pre-processati dal grafo\n",
    "        soggetto_info = neo4j_handler.get_entity_info_from_neo4j(soggetto)\n",
    "        verbo_info = neo4j_handler.get_verb_info_from_neo4j(verbo)\n",
    "        oggetto_info = neo4j_handler.get_entity_info_from_neo4j(oggetto) if oggetto else None\n",
    "        predicato_info = neo4j_handler.get_entity_info_from_neo4j(predicato) if predicato else None\n",
    "\n",
    "        # Generare embeddings per le descrizioni degli elementi\n",
    "        soggetto_description_embeddings = process_description_embeddings(soggetto_info.get('description', []), model)\n",
    "        verbo_description_embeddings = process_description_embeddings(verbo_info.get('description', []), model)\n",
    "        oggetto_description_embeddings = process_description_embeddings(oggetto_info.get('description', []), model) if oggetto_info else []\n",
    "        predicato_description_embeddings = process_description_embeddings(predicato_info.get('description', []), model) if predicato_info else []\n",
    "\n",
    "        # Aggiungi embedding unici\n",
    "        add_unique_embeddings(soggetto_description_embeddings)\n",
    "        add_unique_embeddings(verbo_description_embeddings)\n",
    "        add_unique_embeddings(oggetto_description_embeddings)\n",
    "        add_unique_embeddings(predicato_description_embeddings)\n",
    "\n",
    "    # Processare aggettivi e avverbi\n",
    "    for aggettivo in aggettivi:\n",
    "        aggettivo_info = neo4j_handler.get_adj_adv_info_from_neo4j(aggettivo)\n",
    "        aggettivo_description_embeddings = process_description_embeddings(aggettivo_info.get('description', []), model)\n",
    "        add_unique_embeddings(aggettivo_description_embeddings)\n",
    "\n",
    "    for avverbio in avverbi:\n",
    "        avverbio_info = neo4j_handler.get_adj_adv_info_from_neo4j(avverbio)\n",
    "        avverbio_description_embeddings = process_description_embeddings(avverbio_info.get('description', []), model)\n",
    "        add_unique_embeddings(avverbio_description_embeddings)\n",
    "\n",
    "    # Concatenare tutti i vettori arricchiti\n",
    "    combined_vector = np.concatenate([contextual_embedding, *enriched_embeddings], axis=-1)\n",
    "    \n",
    "    return combined_vector\n",
    "\n",
    "def process_and_save_multivectors(input_file, output_file, neo4j_handler, model, remove_stopwords=False, lemmatize=False):\n",
    "    print(f\"Caricamento del file: {input_file}\")\n",
    "    \n",
    "    # Caricare il dataset arricchito\n",
    "    with open(input_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # Creare un dataframe\n",
    "    df = pd.DataFrame(data)\n",
    "    print(\"Dataframe caricato, avvio elaborazione...\")\n",
    "\n",
    "    # Applicare la funzione di creazione del multivettore a ogni riga del dataset\n",
    "    df['Multivettore'] = df.apply(lambda row: create_multivector_with_svo_and_adj_adv(row, neo4j_handler, model, remove_stopwords, lemmatize), axis=1)\n",
    "    \n",
    "    # Salvare il nuovo dataset con i multivettori inclusi\n",
    "    df.to_json(output_file, orient='records', indent=4)\n",
    "    print(f\"Multivettori salvati in {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bc4910-3297-4779-971b-cfd8a7491c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caricamento del modello Spacy per POS tagging, Dependency Parsing e NER...\n",
      "Modello Spacy caricato.\n",
      "Caricamento del dataset Stereoset (intrasentence)...\n",
      "Dataset Stereoset caricato.\n",
      "Estrazione dei ruoli sintattici, NER e relazioni SVO per tutte le frasi...\n",
      "Inizio popolamento del database Neo4j per la frase: 'Bengali tigers are beautiful.'\n",
      "Frase analizzata.\n",
      "Inizio popolamento del database Neo4j per la frase: 'You can only see them in zoos.'\n",
      "Nessun synset trovato per il termine 'You'\n",
      "Nessun synset trovato per il termine 'them'\n",
      "Frase analizzata.\n",
      "Inizio popolamento del database Neo4j per la frase: 'There are some left in the wild.'\n",
      "Frase analizzata.\n",
      "Inizio popolamento del database Neo4j per la frase: 'He had several assistants with him.'\n",
      "Frase analizzata.\n",
      "Inizio popolamento del database Neo4j per la frase: 'The mall closes at ten o'clock.'\n",
      "Nessun synset trovato per il termine 'ten o'clock'\n",
      "Frase analizzata.\n",
      "Inizio popolamento del database Neo4j per la frase: 'He and the defense lawyer were friends.'\n",
      "Frase analizzata.\n",
      "Inizio popolamento del database Neo4j per la frase: 'He is also studying art history and romance languages.'\n",
      "Frase analizzata.\n",
      "Inizio popolamento del database Neo4j per la frase: 'Michael is a football player in college.'\n",
      "Frase analizzata.\n",
      "Inizio popolamento del database Neo4j per la frase: 'He also belongs to the fraternity and majors in business.'\n",
      "Frase analizzata.\n",
      "Inizio popolamento del database Neo4j per la frase: 'Room temperature water is better for hydration than iced water.'\n",
      "Frase analizzata.\n",
      "Inizio popolamento del database Neo4j per la frase: 'Kenya is in Africa.'\n",
      "Frase analizzata.\n",
      "Inizio popolamento del database Neo4j per la frase: 'Syria is north of Israel.'\n",
      "Frase analizzata.\n",
      "Inizio popolamento del database Neo4j per la frase: 'There is a civil war going on there.'\n",
      "Frase analizzata.\n",
      "Inizio popolamento del database Neo4j per la frase: 'There are places in the country where peace has arrived.'\n",
      "Frase analizzata.\n",
      "Inizio popolamento del database Neo4j per la frase: 'He is a stay at home father'\n",
      "Frase analizzata.\n",
      "Inizio popolamento del database Neo4j per la frase: 'A male is the opposite of a female'\n",
      "Frase analizzata.\n",
      "Inizio popolamento del database Neo4j per la frase: 'He lifts a heavy box of books'\n",
      "Frase analizzata.\n",
      "Inizio popolamento del database Neo4j per la frase: 'I paint my nails red'\n",
      "Frase analizzata.\n",
      "Inizio popolamento del database Neo4j per la frase: 'Many people from here have a red dot on their forehead.'\n",
      "Frase analizzata.\n",
      "Inizio popolamento del database Neo4j per la frase: 'Bangladesh is a country in India.'\n",
      "Frase analizzata.\n",
      "Inizio popolamento del database Neo4j per la frase: 'This place is near the water.'\n",
      "Frase analizzata.\n",
      "Inizio popolamento del database Neo4j per la frase: 'People from here speak good English.'\n"
     ]
    }
   ],
   "source": [
    "enable_neo4j_population = True\n",
    "\n",
    "neo4j_handler = Neo4jHandler(\"bolt://localhost:7687\", \"neo4j\", \"10086832\")\n",
    "\n",
    "print(\"Caricamento del modello Spacy per POS tagging, Dependency Parsing e NER...\")\n",
    "if not 'nlp' in globals():\n",
    "    nlp = spacy.load(\"en_core_web_lg\")\n",
    "    print(\"Modello Spacy caricato.\")\n",
    "\n",
    "# Main logic to load dataset, enrich sentences, and populate Neo4j\n",
    "if not os.path.exists('stereoset_enriched.json'):\n",
    "\n",
    "    print(\"Caricamento del dataset Stereoset (intrasentence)...\")\n",
    "    intrasentence_dataset = load_dataset('McGill-NLP/stereoset', 'intrasentence')\n",
    "    intersentence_dataset = load_dataset('McGill-NLP/stereoset', 'intersentence')\n",
    "    print(\"Dataset Stereoset caricato.\")\n",
    "\n",
    "    data = []\n",
    "    for i, item in enumerate(intrasentence_dataset['validation']):            \n",
    "        context = item['context']\n",
    "        #target = item['target']\n",
    "        #bias_type = item['bias_type']\n",
    "        \n",
    "        for j, sentence in enumerate(item['sentences']['sentence']):\n",
    "            #gold_label = item['sentences']['gold_label'][j]  \n",
    "            data.append({\n",
    "                'Contesto': context,\n",
    "                'Frase': sentence,\n",
    "                #'Target': target,\n",
    "                #'Bias Type': bias_type,\n",
    "                #'Gold Label': gold_label\n",
    "            })\n",
    "\n",
    "    for i, item in enumerate(intersentence_dataset['validation']):            \n",
    "        context = item['context']\n",
    "        #target = item['target']\n",
    "        #bias_type = item['bias_type']\n",
    "        \n",
    "        for j, sentence in enumerate(item['sentences']['sentence']):\n",
    "            #gold_label = item['sentences']['gold_label'][j]  \n",
    "            data.append({\n",
    "                'Contesto': context,\n",
    "                'Frase': sentence,\n",
    "                #'Target': target,\n",
    "                #'Bias Type': bias_type,\n",
    "                #'Gold Label': gold_label\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    #df = df.head(3)\n",
    "\n",
    "    print(\"Estrazione dei ruoli sintattici, NER e relazioni SVO per tutte le frasi...\")\n",
    "    df_enriched = df.apply(lambda row: enrich_sentence(row, enable_neo4j_population), axis=1)\n",
    "    print(\"Estrazione completata.\")\n",
    "    \n",
    "    df_combined = pd.concat([df, df_enriched], axis=1)\n",
    "    df_combined.to_json('stereoset_enriched.json', orient='records', indent=4)\n",
    "    print(f\"Dataset arricchito con POS, NER, relazioni SVO salvato in 'stereoset_enriched.json'.\")\n",
    "\n",
    "    neo4j_handler.close()\n",
    "else:\n",
    "    # Caricamento del modello BERT e tokenizer per l'embedding delle frasi\n",
    "    #tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    #model = AutoModel.from_pretrained(\"bert-base-uncased\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = SentenceTransformer('paraphrase-mpnet-base-v2')\n",
    "    \n",
    "    input_file = 'stereoset_enriched.json'\n",
    "    print(f\"Caricamento del file JSON arricchito 'stereoset_enriched.json'\")\n",
    "    with open(input_file, 'r') as stereoset_file:\n",
    "        df_final = json.load(stereoset_file)\n",
    "    \n",
    "    output_file = 'stereoset_with_multivectors_for_clustering.json'\n",
    "    process_and_save_multivectors(input_file, output_file, neo4j_handler, model, True, True)\n",
    "    \n",
    "    neo4j_handler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b028e6c-bbf6-470a-a152-d47698cf8969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a5e056-9b5c-4b68-8e57-36e3350a8a7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
